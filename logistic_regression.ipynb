{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "logistic_regression.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "JndnmDMp66FL",
        "dPpJUV862FYI",
        "i2e3TlyL57Qs",
        "wCugvl0JdWYL"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/vickode/vickode/blob/master/logistic_regression.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "JndnmDMp66FL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Copyright 2017 Google LLC."
      ]
    },
    {
      "metadata": {
        "id": "hMqWDc_m6rUC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g4T-_IsVbweU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression"
      ]
    },
    {
      "metadata": {
        "id": "LEAHZv4rIYHX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Learning Objectives:**\n",
        "  * Reframe the median house value predictor (from the preceding exercises) as a binary classification model\n",
        "  * Compare the effectiveness of logisitic regression vs linear regression for a binary classification problem"
      ]
    },
    {
      "metadata": {
        "id": "CnkCZqdIIYHY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As in the prior exercises, we're working with the [California housing data set](https://developers.google.com/machine-learning/crash-course/california-housing-data-description), but this time we will turn it into a binary classification problem by predicting whether a city block is a high-cost city block. We'll also revert to the default features, for now."
      ]
    },
    {
      "metadata": {
        "id": "9pltCyy2K3dd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Frame the Problem as Binary Classification\n",
        "\n",
        "The target of our dataset is `median_house_value` which is a numeric (continuous-valued) feature. We can create a boolean label by applying a threshold to this continuous value.\n",
        "\n",
        "Given features describing a city block, we wish to predict if it is a high-cost city block. To prepare the targets for train and eval data, we define a classification threshold of the 75%-ile for median house value (a value of approximately 265000). All house values above the threshold are labeled `1`, and all others are labeled `0`."
      ]
    },
    {
      "metadata": {
        "id": "67IJwZX1Vvjt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "\n",
        "Run the cells below to load the data and prepare the input features and targets."
      ]
    },
    {
      "metadata": {
        "id": "fOlbcJ4EIYHd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "outputId": "f281a927-470a-42a4-9cd7-8b8e1f416be2"
      },
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "from IPython import display\n",
        "from matplotlib import cm\n",
        "from matplotlib import gridspec\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import metrics\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.data import Dataset\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "pd.options.display.max_rows = 10\n",
        "pd.options.display.float_format = '{:.1f}'.format\n",
        "\n",
        "california_housing_dataframe = pd.read_csv(\"https://storage.googleapis.com/mledu-datasets/california_housing_train.csv\", sep=\",\")\n",
        "\n",
        "california_housing_dataframe = california_housing_dataframe.reindex(\n",
        "    np.random.permutation(california_housing_dataframe.index))\n",
        "\n",
        "california_housing_dataframe"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>longitude</th>\n",
              "      <th>latitude</th>\n",
              "      <th>housing_median_age</th>\n",
              "      <th>total_rooms</th>\n",
              "      <th>total_bedrooms</th>\n",
              "      <th>population</th>\n",
              "      <th>households</th>\n",
              "      <th>median_income</th>\n",
              "      <th>median_house_value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>12534</th>\n",
              "      <td>-121.6</td>\n",
              "      <td>36.7</td>\n",
              "      <td>17.0</td>\n",
              "      <td>4203.0</td>\n",
              "      <td>816.0</td>\n",
              "      <td>2900.0</td>\n",
              "      <td>827.0</td>\n",
              "      <td>4.2</td>\n",
              "      <td>159900.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13845</th>\n",
              "      <td>-122.0</td>\n",
              "      <td>37.6</td>\n",
              "      <td>31.0</td>\n",
              "      <td>2155.0</td>\n",
              "      <td>522.0</td>\n",
              "      <td>1858.0</td>\n",
              "      <td>437.0</td>\n",
              "      <td>2.7</td>\n",
              "      <td>159800.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2173</th>\n",
              "      <td>-117.4</td>\n",
              "      <td>33.2</td>\n",
              "      <td>35.0</td>\n",
              "      <td>2204.0</td>\n",
              "      <td>482.0</td>\n",
              "      <td>1435.0</td>\n",
              "      <td>462.0</td>\n",
              "      <td>3.7</td>\n",
              "      <td>125600.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5623</th>\n",
              "      <td>-118.2</td>\n",
              "      <td>34.1</td>\n",
              "      <td>27.0</td>\n",
              "      <td>2025.0</td>\n",
              "      <td>565.0</td>\n",
              "      <td>2189.0</td>\n",
              "      <td>577.0</td>\n",
              "      <td>2.6</td>\n",
              "      <td>148600.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16633</th>\n",
              "      <td>-122.7</td>\n",
              "      <td>38.5</td>\n",
              "      <td>16.0</td>\n",
              "      <td>1834.0</td>\n",
              "      <td>391.0</td>\n",
              "      <td>994.0</td>\n",
              "      <td>390.0</td>\n",
              "      <td>3.7</td>\n",
              "      <td>156500.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8180</th>\n",
              "      <td>-118.4</td>\n",
              "      <td>35.1</td>\n",
              "      <td>21.0</td>\n",
              "      <td>1899.0</td>\n",
              "      <td>447.0</td>\n",
              "      <td>1133.0</td>\n",
              "      <td>391.0</td>\n",
              "      <td>1.9</td>\n",
              "      <td>67900.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15733</th>\n",
              "      <td>-122.4</td>\n",
              "      <td>37.8</td>\n",
              "      <td>52.0</td>\n",
              "      <td>1182.0</td>\n",
              "      <td>307.0</td>\n",
              "      <td>1029.0</td>\n",
              "      <td>306.0</td>\n",
              "      <td>2.1</td>\n",
              "      <td>214600.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4183</th>\n",
              "      <td>-118.0</td>\n",
              "      <td>33.8</td>\n",
              "      <td>19.0</td>\n",
              "      <td>1991.0</td>\n",
              "      <td>528.0</td>\n",
              "      <td>1202.0</td>\n",
              "      <td>460.0</td>\n",
              "      <td>3.2</td>\n",
              "      <td>252100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7436</th>\n",
              "      <td>-118.3</td>\n",
              "      <td>33.9</td>\n",
              "      <td>26.0</td>\n",
              "      <td>2159.0</td>\n",
              "      <td>523.0</td>\n",
              "      <td>1331.0</td>\n",
              "      <td>520.0</td>\n",
              "      <td>3.9</td>\n",
              "      <td>264500.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14028</th>\n",
              "      <td>-122.0</td>\n",
              "      <td>38.6</td>\n",
              "      <td>20.0</td>\n",
              "      <td>1005.0</td>\n",
              "      <td>168.0</td>\n",
              "      <td>457.0</td>\n",
              "      <td>157.0</td>\n",
              "      <td>5.7</td>\n",
              "      <td>225000.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>17000 rows × 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
              "12534     -121.6      36.7                17.0       4203.0           816.0   \n",
              "13845     -122.0      37.6                31.0       2155.0           522.0   \n",
              "2173      -117.4      33.2                35.0       2204.0           482.0   \n",
              "5623      -118.2      34.1                27.0       2025.0           565.0   \n",
              "16633     -122.7      38.5                16.0       1834.0           391.0   \n",
              "...          ...       ...                 ...          ...             ...   \n",
              "8180      -118.4      35.1                21.0       1899.0           447.0   \n",
              "15733     -122.4      37.8                52.0       1182.0           307.0   \n",
              "4183      -118.0      33.8                19.0       1991.0           528.0   \n",
              "7436      -118.3      33.9                26.0       2159.0           523.0   \n",
              "14028     -122.0      38.6                20.0       1005.0           168.0   \n",
              "\n",
              "       population  households  median_income  median_house_value  \n",
              "12534      2900.0       827.0            4.2            159900.0  \n",
              "13845      1858.0       437.0            2.7            159800.0  \n",
              "2173       1435.0       462.0            3.7            125600.0  \n",
              "5623       2189.0       577.0            2.6            148600.0  \n",
              "16633       994.0       390.0            3.7            156500.0  \n",
              "...           ...         ...            ...                 ...  \n",
              "8180       1133.0       391.0            1.9             67900.0  \n",
              "15733      1029.0       306.0            2.1            214600.0  \n",
              "4183       1202.0       460.0            3.2            252100.0  \n",
              "7436       1331.0       520.0            3.9            264500.0  \n",
              "14028       457.0       157.0            5.7            225000.0  \n",
              "\n",
              "[17000 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "lTB73MNeIYHf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Note how the code below is slightly different from the previous exercises. Instead of using `median_house_value` as target, we create a new binary target, `median_house_value_is_high`."
      ]
    },
    {
      "metadata": {
        "id": "kPSqspaqIYHg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def preprocess_features(california_housing_dataframe):\n",
        "  \"\"\"Prepares input features from California housing data set.\n",
        "\n",
        "  Args:\n",
        "    california_housing_dataframe: A Pandas DataFrame expected to contain data\n",
        "      from the California housing data set.\n",
        "  Returns:\n",
        "    A DataFrame that contains the features to be used for the model, including\n",
        "    synthetic features.\n",
        "  \"\"\"\n",
        "  selected_features = california_housing_dataframe[\n",
        "    [\"latitude\",\n",
        "     \"longitude\",\n",
        "     \"housing_median_age\",\n",
        "     \"total_rooms\",\n",
        "     \"total_bedrooms\",\n",
        "     \"population\",\n",
        "     \"households\",\n",
        "     \"median_income\"]]\n",
        "  processed_features = selected_features.copy()\n",
        "  # Create a synthetic feature.\n",
        "  processed_features[\"rooms_per_person\"] = (\n",
        "    california_housing_dataframe[\"total_rooms\"] /\n",
        "    california_housing_dataframe[\"population\"])\n",
        "  return processed_features\n",
        "\n",
        "def preprocess_targets(california_housing_dataframe):\n",
        "  \"\"\"Prepares target features (i.e., labels) from California housing data set.\n",
        "\n",
        "  Args:\n",
        "    california_housing_dataframe: A Pandas DataFrame expected to contain data\n",
        "      from the California housing data set.\n",
        "  Returns:\n",
        "    A DataFrame that contains the target feature.\n",
        "  \"\"\"\n",
        "  output_targets = pd.DataFrame()\n",
        "  # Create a boolean categorical feature representing whether the\n",
        "  # median_house_value is above a set threshold.\n",
        "  output_targets[\"median_house_value_is_high\"] = (\n",
        "    california_housing_dataframe[\"median_house_value\"] > 265000).astype(float)\n",
        "  return output_targets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FwOYWmXqWA6D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1205
        },
        "outputId": "e59cbe8c-2e4c-40f4-896c-8a2fc215e3c9"
      },
      "cell_type": "code",
      "source": [
        "# Choose the first 12000 (out of 17000) examples for training.\n",
        "training_examples = preprocess_features(california_housing_dataframe.head(12000))\n",
        "training_targets = preprocess_targets(california_housing_dataframe.head(12000))\n",
        "\n",
        "# Choose the last 5000 (out of 17000) examples for validation.\n",
        "validation_examples = preprocess_features(california_housing_dataframe.tail(5000))\n",
        "validation_targets = preprocess_targets(california_housing_dataframe.tail(5000))\n",
        "\n",
        "# Double-check that we've done the right thing.\n",
        "print \"Training examples summary:\"\n",
        "display.display(training_examples.describe())\n",
        "print \"Validation examples summary:\"\n",
        "display.display(validation_examples.describe())\n",
        "\n",
        "print \"Training targets summary:\"\n",
        "display.display(training_targets.describe())\n",
        "print \"Validation targets summary:\"\n",
        "display.display(validation_targets.describe())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training examples summary:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "       latitude  longitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
              "count   12000.0    12000.0             12000.0      12000.0         12000.0   \n",
              "mean       35.6     -119.6                28.5       2654.2           540.4   \n",
              "std         2.1        2.0                12.6       2181.5           421.1   \n",
              "min        32.5     -124.3                 1.0          2.0             1.0   \n",
              "25%        33.9     -121.8                18.0       1469.8           297.0   \n",
              "50%        34.3     -118.5                29.0       2125.5           431.0   \n",
              "75%        37.7     -118.0                37.0       3167.0           650.0   \n",
              "max        41.9     -114.3                52.0      32054.0          5290.0   \n",
              "\n",
              "       population  households  median_income  rooms_per_person  \n",
              "count     12000.0     12000.0        12000.0           12000.0  \n",
              "mean       1430.3       501.9            3.9               2.0  \n",
              "std        1144.3       383.2            1.9               1.2  \n",
              "min           3.0         1.0            0.5               0.0  \n",
              "25%         790.0       282.0            2.6               1.5  \n",
              "50%        1166.0       407.0            3.6               1.9  \n",
              "75%        1722.2       605.0            4.8               2.3  \n",
              "max       35682.0      5050.0           15.0              55.2  "
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>latitude</th>\n",
              "      <th>longitude</th>\n",
              "      <th>housing_median_age</th>\n",
              "      <th>total_rooms</th>\n",
              "      <th>total_bedrooms</th>\n",
              "      <th>population</th>\n",
              "      <th>households</th>\n",
              "      <th>median_income</th>\n",
              "      <th>rooms_per_person</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>12000.0</td>\n",
              "      <td>12000.0</td>\n",
              "      <td>12000.0</td>\n",
              "      <td>12000.0</td>\n",
              "      <td>12000.0</td>\n",
              "      <td>12000.0</td>\n",
              "      <td>12000.0</td>\n",
              "      <td>12000.0</td>\n",
              "      <td>12000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>35.6</td>\n",
              "      <td>-119.6</td>\n",
              "      <td>28.5</td>\n",
              "      <td>2654.2</td>\n",
              "      <td>540.4</td>\n",
              "      <td>1430.3</td>\n",
              "      <td>501.9</td>\n",
              "      <td>3.9</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>2.1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>12.6</td>\n",
              "      <td>2181.5</td>\n",
              "      <td>421.1</td>\n",
              "      <td>1144.3</td>\n",
              "      <td>383.2</td>\n",
              "      <td>1.9</td>\n",
              "      <td>1.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>32.5</td>\n",
              "      <td>-124.3</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>33.9</td>\n",
              "      <td>-121.8</td>\n",
              "      <td>18.0</td>\n",
              "      <td>1469.8</td>\n",
              "      <td>297.0</td>\n",
              "      <td>790.0</td>\n",
              "      <td>282.0</td>\n",
              "      <td>2.6</td>\n",
              "      <td>1.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>34.3</td>\n",
              "      <td>-118.5</td>\n",
              "      <td>29.0</td>\n",
              "      <td>2125.5</td>\n",
              "      <td>431.0</td>\n",
              "      <td>1166.0</td>\n",
              "      <td>407.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>37.7</td>\n",
              "      <td>-118.0</td>\n",
              "      <td>37.0</td>\n",
              "      <td>3167.0</td>\n",
              "      <td>650.0</td>\n",
              "      <td>1722.2</td>\n",
              "      <td>605.0</td>\n",
              "      <td>4.8</td>\n",
              "      <td>2.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>41.9</td>\n",
              "      <td>-114.3</td>\n",
              "      <td>52.0</td>\n",
              "      <td>32054.0</td>\n",
              "      <td>5290.0</td>\n",
              "      <td>35682.0</td>\n",
              "      <td>5050.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>55.2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Validation examples summary:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "       latitude  longitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
              "count    5000.0     5000.0              5000.0       5000.0          5000.0   \n",
              "mean       35.6     -119.5                28.8       2618.3           537.0   \n",
              "std         2.1        2.0                12.5       2176.1           422.4   \n",
              "min        32.5     -124.2                 1.0         12.0             3.0   \n",
              "25%        33.9     -121.7                19.0       1444.8           295.0   \n",
              "50%        34.2     -118.5                29.0       2136.5           439.0   \n",
              "75%        37.7     -118.0                37.0       3112.2           646.0   \n",
              "max        42.0     -114.6                52.0      37937.0          6445.0   \n",
              "\n",
              "       population  households  median_income  rooms_per_person  \n",
              "count      5000.0      5000.0         5000.0            5000.0  \n",
              "mean       1427.7       499.6            3.8               2.0  \n",
              "std        1156.5       387.6            1.9               1.0  \n",
              "min           9.0         3.0            0.5               0.2  \n",
              "25%         787.8       280.8            2.6               1.5  \n",
              "50%        1171.5       414.0            3.5               1.9  \n",
              "75%        1718.0       606.2            4.7               2.3  \n",
              "max       28566.0      6082.0           15.0              29.4  "
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>latitude</th>\n",
              "      <th>longitude</th>\n",
              "      <th>housing_median_age</th>\n",
              "      <th>total_rooms</th>\n",
              "      <th>total_bedrooms</th>\n",
              "      <th>population</th>\n",
              "      <th>households</th>\n",
              "      <th>median_income</th>\n",
              "      <th>rooms_per_person</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>5000.0</td>\n",
              "      <td>5000.0</td>\n",
              "      <td>5000.0</td>\n",
              "      <td>5000.0</td>\n",
              "      <td>5000.0</td>\n",
              "      <td>5000.0</td>\n",
              "      <td>5000.0</td>\n",
              "      <td>5000.0</td>\n",
              "      <td>5000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>35.6</td>\n",
              "      <td>-119.5</td>\n",
              "      <td>28.8</td>\n",
              "      <td>2618.3</td>\n",
              "      <td>537.0</td>\n",
              "      <td>1427.7</td>\n",
              "      <td>499.6</td>\n",
              "      <td>3.8</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>2.1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>12.5</td>\n",
              "      <td>2176.1</td>\n",
              "      <td>422.4</td>\n",
              "      <td>1156.5</td>\n",
              "      <td>387.6</td>\n",
              "      <td>1.9</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>32.5</td>\n",
              "      <td>-124.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>33.9</td>\n",
              "      <td>-121.7</td>\n",
              "      <td>19.0</td>\n",
              "      <td>1444.8</td>\n",
              "      <td>295.0</td>\n",
              "      <td>787.8</td>\n",
              "      <td>280.8</td>\n",
              "      <td>2.6</td>\n",
              "      <td>1.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>34.2</td>\n",
              "      <td>-118.5</td>\n",
              "      <td>29.0</td>\n",
              "      <td>2136.5</td>\n",
              "      <td>439.0</td>\n",
              "      <td>1171.5</td>\n",
              "      <td>414.0</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>37.7</td>\n",
              "      <td>-118.0</td>\n",
              "      <td>37.0</td>\n",
              "      <td>3112.2</td>\n",
              "      <td>646.0</td>\n",
              "      <td>1718.0</td>\n",
              "      <td>606.2</td>\n",
              "      <td>4.7</td>\n",
              "      <td>2.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>42.0</td>\n",
              "      <td>-114.6</td>\n",
              "      <td>52.0</td>\n",
              "      <td>37937.0</td>\n",
              "      <td>6445.0</td>\n",
              "      <td>28566.0</td>\n",
              "      <td>6082.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>29.4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Training targets summary:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "       median_house_value_is_high\n",
              "count                     12000.0\n",
              "mean                          0.3\n",
              "std                           0.4\n",
              "min                           0.0\n",
              "25%                           0.0\n",
              "50%                           0.0\n",
              "75%                           1.0\n",
              "max                           1.0"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>median_house_value_is_high</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>12000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Validation targets summary:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "       median_house_value_is_high\n",
              "count                      5000.0\n",
              "mean                          0.2\n",
              "std                           0.4\n",
              "min                           0.0\n",
              "25%                           0.0\n",
              "50%                           0.0\n",
              "75%                           0.0\n",
              "max                           1.0"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>median_house_value_is_high</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>5000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "uon1LB3A31VN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## How Would Linear Regression Fare?\n",
        "To see why logistic regression is effective, let us first train a naive model that uses linear regression. This model will use labels with values in the set `{0, 1}` and will try to predict a continuous value that is as close as possible to `0` or `1`. Furthermore, we wish to interpret the output as a probability, so it would be ideal if the output will be within the range `(0, 1)`. We would then apply a threshold of `0.5` to determine the label.\n",
        "\n",
        "Run the cells below to train the linear regression model using [LinearRegressor](https://www.tensorflow.org/api_docs/python/tf/estimator/LinearRegressor)."
      ]
    },
    {
      "metadata": {
        "id": "smmUYRDtWOV_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def construct_feature_columns(input_features):\n",
        "  \"\"\"Construct the TensorFlow Feature Columns.\n",
        "\n",
        "  Args:\n",
        "    input_features: The names of the numerical input features to use.\n",
        "  Returns:\n",
        "    A set of feature columns\n",
        "  \"\"\"\n",
        "  return set([tf.feature_column.numeric_column(my_feature)\n",
        "              for my_feature in input_features])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B5OwSrr1yIKD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def my_input_fn(features, targets, batch_size=1, shuffle=True, num_epochs=None):\n",
        "    \"\"\"Trains a linear regression model.\n",
        "  \n",
        "    Args:\n",
        "      features: pandas DataFrame of features\n",
        "      targets: pandas DataFrame of targets\n",
        "      batch_size: Size of batches to be passed to the model\n",
        "      shuffle: True or False. Whether to shuffle the data.\n",
        "      num_epochs: Number of epochs for which data should be repeated. None = repeat indefinitely\n",
        "    Returns:\n",
        "      Tuple of (features, labels) for next data batch\n",
        "    \"\"\"\n",
        "    \n",
        "    # Convert pandas data into a dict of np arrays.\n",
        "    features = {key:np.array(value) for key,value in dict(features).items()}                                            \n",
        " \n",
        "    # Construct a dataset, and configure batching/repeating.\n",
        "    ds = Dataset.from_tensor_slices((features,targets)) # warning: 2GB limit\n",
        "    ds = ds.batch(batch_size).repeat(num_epochs)\n",
        "    \n",
        "    # Shuffle the data, if specified.\n",
        "    if shuffle:\n",
        "      ds = ds.shuffle(10000)\n",
        "    \n",
        "    # Return the next batch of data.\n",
        "    features, labels = ds.make_one_shot_iterator().get_next()\n",
        "    return features, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SE2-hq8PIYHz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_linear_regressor_model(\n",
        "    learning_rate,\n",
        "    steps,\n",
        "    batch_size,\n",
        "    training_examples,\n",
        "    training_targets,\n",
        "    validation_examples,\n",
        "    validation_targets):\n",
        "  \"\"\"Trains a linear regression model.\n",
        "  \n",
        "  In addition to training, this function also prints training progress information,\n",
        "  as well as a plot of the training and validation loss over time.\n",
        "  \n",
        "  Args:\n",
        "    learning_rate: A `float`, the learning rate.\n",
        "    steps: A non-zero `int`, the total number of training steps. A training step\n",
        "      consists of a forward and backward pass using a single batch.\n",
        "    batch_size: A non-zero `int`, the batch size.\n",
        "    training_examples: A `DataFrame` containing one or more columns from\n",
        "      `california_housing_dataframe` to use as input features for training.\n",
        "    training_targets: A `DataFrame` containing exactly one column from\n",
        "      `california_housing_dataframe` to use as target for training.\n",
        "    validation_examples: A `DataFrame` containing one or more columns from\n",
        "      `california_housing_dataframe` to use as input features for validation.\n",
        "    validation_targets: A `DataFrame` containing exactly one column from\n",
        "      `california_housing_dataframe` to use as target for validation.\n",
        "      \n",
        "  Returns:\n",
        "    A `LinearRegressor` object trained on the training data.\n",
        "  \"\"\"\n",
        "\n",
        "  periods = 10\n",
        "  steps_per_period = steps / periods\n",
        "\n",
        "  # Create a linear regressor object.\n",
        "  my_optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
        "  my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)\n",
        "  linear_regressor = tf.estimator.LinearRegressor(\n",
        "      feature_columns=construct_feature_columns(training_examples),\n",
        "      optimizer=my_optimizer\n",
        "  )\n",
        "    \n",
        "  # Create input functions.\n",
        "  training_input_fn = lambda: my_input_fn(training_examples, \n",
        "                                          training_targets[\"median_house_value_is_high\"], \n",
        "                                          batch_size=batch_size)\n",
        "  predict_training_input_fn = lambda: my_input_fn(training_examples, \n",
        "                                                  training_targets[\"median_house_value_is_high\"], \n",
        "                                                  num_epochs=1, \n",
        "                                                  shuffle=False)\n",
        "  predict_validation_input_fn = lambda: my_input_fn(validation_examples, \n",
        "                                                    validation_targets[\"median_house_value_is_high\"], \n",
        "                                                    num_epochs=1, \n",
        "                                                    shuffle=False)\n",
        "\n",
        "  # Train the model, but do so inside a loop so that we can periodically assess\n",
        "  # loss metrics.\n",
        "  print \"Training model...\"\n",
        "  print \"RMSE (on training data):\"\n",
        "  training_rmse = []\n",
        "  validation_rmse = []\n",
        "  for period in range (0, periods):\n",
        "    # Train the model, starting from the prior state.\n",
        "    linear_regressor.train(\n",
        "        input_fn=training_input_fn,\n",
        "        steps=steps_per_period\n",
        "    )\n",
        "    \n",
        "    # Take a break and compute predictions.\n",
        "    training_predictions = linear_regressor.predict(input_fn=predict_training_input_fn)\n",
        "    training_predictions = np.array([item['predictions'][0] for item in training_predictions])\n",
        "    \n",
        "    validation_predictions = linear_regressor.predict(input_fn=predict_validation_input_fn)\n",
        "    validation_predictions = np.array([item['predictions'][0] for item in validation_predictions])\n",
        "    \n",
        "    # Compute training and validation loss.\n",
        "    training_root_mean_squared_error = math.sqrt(\n",
        "        metrics.mean_squared_error(training_predictions, training_targets))\n",
        "    validation_root_mean_squared_error = math.sqrt(\n",
        "        metrics.mean_squared_error(validation_predictions, validation_targets))\n",
        "    # Occasionally print the current loss.\n",
        "    print \"  period %02d : %0.2f\" % (period, training_root_mean_squared_error)\n",
        "    # Add the loss metrics from this period to our list.\n",
        "    training_rmse.append(training_root_mean_squared_error)\n",
        "    validation_rmse.append(validation_root_mean_squared_error)\n",
        "  print \"Model training finished.\"\n",
        "  \n",
        "  # Output a graph of loss metrics over periods.\n",
        "  plt.ylabel(\"RMSE\")\n",
        "  plt.xlabel(\"Periods\")\n",
        "  plt.title(\"Root Mean Squared Error vs. Periods\")\n",
        "  plt.tight_layout()\n",
        "  plt.plot(training_rmse, label=\"training\")\n",
        "  plt.plot(validation_rmse, label=\"validation\")\n",
        "  plt.legend()\n",
        "\n",
        "  return linear_regressor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TDBD8xeeIYH2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 622
        },
        "outputId": "e07e0436-ddb9-4dea-8d49-49fafa02a15c"
      },
      "cell_type": "code",
      "source": [
        "linear_regressor = train_linear_regressor_model(\n",
        "    learning_rate=0.000001,\n",
        "    steps=200,\n",
        "    batch_size=20,\n",
        "    training_examples=training_examples,\n",
        "    training_targets=training_targets,\n",
        "    validation_examples=validation_examples,\n",
        "    validation_targets=validation_targets)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training model...\n",
            "RMSE (on training data):\n",
            "  period 00 : 0.45\n",
            "  period 01 : 0.46\n",
            "  period 02 : 0.45\n",
            "  period 03 : 0.44\n",
            "  period 04 : 0.44\n",
            "  period 05 : 0.45\n",
            "  period 06 : 0.44\n",
            "  period 07 : 0.44\n",
            "  period 08 : 0.44\n",
            "  period 09 : 0.44\n",
            "Model training finished.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAj4AAAGACAYAAACtGmg0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzs3XlclWX6+PHPgQOyIzsCKoiaigou\n4AIquOKWmUuWUVN969diOWZT6WTaVGbT1ExmNtNMWTk1WUZuWZoL7oCKK64gyio7h3075/n9YZ4i\nFeFwDiBc79er18tzznPfz/WcG+Pyfq7nvlWKoigIIYQQQrQDZi0dgBBCCCFEc5HERwghhBDthiQ+\nQgghhGg3JPERQgghRLshiY8QQggh2g1JfIQQQgjRbqhbOgAh2qK77rqLLl26YG5uDoBWqyU4OJhX\nXnkFGxsbg/v95ptvmD179g3vR0dHs2jRIv75z38SERGhf7+yspLhw4czfvx4VqxYYfB5Gyo1NZXl\ny5eTkpICgLW1NfPmzWPs2LEmP3djrF69mtTU1Bu+k7i4OB577DF8fHxuaPPTTz81V3hNkp6ezpgx\nY/Dz8wNAURRcXV3585//TJ8+fRrV17vvvouXlxf3339/g9ts3LiR9evXs3bt2kadS4jmIomPECay\ndu1aPD09AaiurmbBggX861//YsGCBQb1l5uby3/+85+bJj4AnTp1YsuWLXUSn927d+Pg4GDQ+Qzx\nwgsvMG3aNP75z38CcOLECR5++GF+/PFHOnXq1GxxNEWnTp3umCTnVszNzetcw9atW3nmmWfYtm0b\nlpaWDe5n4cKFpghPiBYlt7qEaAaWlpaMGDGCs2fPAlBVVcWrr77KhAkTmDhxIitWrECr1QJw7tw5\n5syZQ2RkJNOmTWPfvn0AzJkzh8zMTCIjI6murr7hHAMHDiQuLo6Kigr9e1u3biU0NFT/urq6mjfe\neIMJEyYwevRofYICcOzYMe69914iIyOZNGkSBw8eBK7NIISFhfHFF18wdepURowYwdatW296nRcu\nXCAwMFD/OjAwkG3btukTwFWrVjFq1CjuuecePv74Y0aPHg3Ayy+/zOrVq/Xtfvv6dnEtX76cBx98\nEICjR48yY8YMxo0bx+zZs0lLSwOuzXz98Y9/JCIiggcffJCrV6/eZsRuLjo6mnnz5vHwww/z17/+\nlbi4OObMmcP8+fP1ScKPP/7IlClTiIyM5KGHHiI1NRWADz74gFdeeYWZM2fy2Wef1el3/vz5fPrp\np/rXZ8+eJSwsDJ1Ox9///ncmTJjAhAkTeOihh8jOzm503JMmTaKyspJLly4BsG7dOiIjIxk9ejTP\nP/88lZWVwLXv/a233mLq1Kn8+OOPdcbhVj+XOp2Ov/zlL4SHhzNz5kzOnTunP298fDzTp09n0qRJ\nTJw4kR9//LHRsQthdIoQwuh69uypZGVl6V8XFRUpc+fOVVavXq0oiqL861//Uh5//HGlpqZGqaio\nUGbMmKFs2LBB0Wq1ysSJE5XNmzcriqIoJ0+eVIKDg5WSkhIlNjZWGTt27E3P99133ykvvfSS8sIL\nL+jblpSUKGPGjFG+/fZb5aWXXlIURVFWrVqlPPzww0pVVZVSVlam3HPPPcquXbsURVGUKVOmKFu2\nbFEURVG+//57/bnS0tKUPn36KGvXrlUURVG2bt2qjBs37qZxPPvss0pERITy+eefK0lJSXU+O3/+\nvDJ48GAlJydHqampUZ566iklIiJCURRFeemll5QPP/xQf+xvX9cXV0BAgBIdHa2/3uDgYGX//v2K\noijK5s2blenTpyuKoij//e9/lblz5yo1NTVKQUGBEhERof9Ofqu+7/j69xwUFKSkpKToj+/Xr59y\n8OBBRVEUJSMjQxk0aJBy+fJlRVEU5ZNPPlEefvhhRVEUZeXKlUpYWJiSn59/Q78//PCDMnfuXP3r\n999/X3n99deVCxcuKOPHj1eqq6sVRVGUL774Qvn+++9vGd/176V37943vB8cHKwkJycrhw8fVoYN\nG6ZcvXpVURRFWbJkibJixQpFUa5971OnTlUqKyv1rz/88MN6fy5jYmKU8ePHK6WlpUpFRYUyc+ZM\n5cEHH1QURVHuvfdeJS4uTlEURUlJSVGef/75emMXojnIjI8QJhIVFUVkZCRjxoxhzJgxDB06lMcf\nfxyAmJgYZs+ejVqtxsrKiqlTp3LgwAHS09PJy8tj8uTJAPTr1w8vLy9OnTrVoHNOnjyZLVu2ALBj\nxw4iIiIwM/v1r/nu3bt54IEHsLS0xMbGhmnTprF9+3YANmzYwMSJEwEYNGiQfrYEoLa2lnvvvReA\ngIAAMjMzb3r+d955h7lz57J582amTJnC6NGj+d///gdcm40JDg7Gzc0NtVrNlClTGnRN9cVVU1PD\nuHHj9P17eHjoZ7imTJlCamoqmZmZHDlyhHHjxqFWq3FycqpzO/D3srKyiIyMrPPfb2uBfH198fX1\n1b+2srJi2LBhABw4cIAhQ4bQtWtXAGbNmkVcXBy1tbXAtRkwZ2fnG84ZHh7OmTNnKCoqAuDnn38m\nMjISBwcHCgoK2Lx5MxqNhqioKO65554GfW/XKYrCunXr8PDwwNfXl127djFp0iQ8PDwAuP/++/U/\nAwDDhg2jQ4cOdfqo7+fy8OHDjBo1CltbW6ysrPRjBeDi4sKGDRtITk7G19eXd999t1GxC2EKUuMj\nhIlcr/EpKCjQ36ZRq6/9lSsoKMDR0VF/rKOjI/n5+RQUFGBvb49KpdJ/dv2Xn6ur623PGRoayiuv\nvEJRURE//PADTz/9tL7QGKCkpIS33nqL9957D7h266t///4AbN68mS+++IKysjJ0Oh3Kb7bxMzc3\n1xdlm5mZodPpbnr+Dh068Nhjj/HYY49RXFzMTz/9xPLly/Hx8UGj0dSpN3Jxcbnt9TQkLjs7OwCK\ni4tJS0sjMjJS/7mlpSUFBQVoNBrs7e317zs4OFBWVnbT892uxue34/b714WFhXWu0d7eHkVRKCws\nvGnb62xsbBg+fDgxMTEMGjSI4uJiBg0ahEql4oMPPuDTTz/l9ddfJzg4mNdee+229VJarVb/PSiK\nQvfu3Vm9ejVmZmaUlJTw888/s3//fv3nNTU1t7w+oN6fS41Gg7u7e533r1u+fDkfffQRjzzyCFZW\nVjz//PN1xkeIliCJjxAm5uzsTFRUFO+88w4fffQRAK6urvp/3QMUFRXh6uqKi4sLGo0GRVH0v2SK\niooanCRYWFgQERHBhg0buHLlCgMGDKiT+Li7u/Poo4/eMOORnZ3NK6+8wrfffkvv3r25fPkyEyZM\naNR1FhQUcPbsWf2Mi4ODA7Nnz2bfvn1cuHABe3t7SkpK6hx/3e+TKY1G0+i43N3d6datG9HR0Td8\n5uDgcMtzG5OLiwvHjh3Tv9ZoNJiZmeHk5HTbthMmTODnn3+msLCQCRMm6Md/6NChDB06lPLyct5+\n+23+9re/3Xbm5PfFzb/l7u7O9OnTeemllxp1Xbf6uazvu3V1dWXJkiUsWbKE/fv38+yzzzJixAhs\nbW0bfG4hjE1udQnRDB555BGOHTtGfHw8cO3Wxvr169FqtZSXl7Nx40ZGjRqFj48Pnp6e+uLhhIQE\n8vLy6N+/P2q1mvLycv1tk1uZPHky//73v2/6CPmYMWP49ttv0Wq1KIrC6tWr2bt3LwUFBdjY2NCt\nWzdqa2tZt24dwC1nRW6msrKS5557Tl/0CnDlyhVOnDjB4MGDGTBgAEeOHKGgoIDa2lo2bNigP87N\nzU1fFJuWlkZCQgJAo+IKDAwkNzeXEydO6Pv505/+hKIoBAUFsWvXLrRaLQUFBezdu7fB19UYoaGh\nHDlyRH877uuvvyY0NFQ/01efiIgIjh07xo4dO/S3i/bv389rr72GTqfDxsaGXr161Zl1McTo0aPZ\nvn27PkHZsWMHH3/8cb1t6vu5HDBgAPv376eiooKKigp9wlVTU0NUVBQ5OTnAtVukarW6zq1XIVqC\nzPgI0Qzs7Ox44oknePvtt1m/fj1RUVGkpaUxefJkVCoVkZGRTJw4EZVKxXvvvcfSpUtZtWoV1tbW\nvP/++9jY2HDXXXfh6OhIaGgo33//PV5eXjc9V0hICCqVikmTJt3w2QMPPEB6ejqTJ09GURT69u3L\nww8/jI2NDSNHjmTChAm4uLjw8ssvk5CQQFRUFCtXrmzQNXp5efHRRx+xcuVK3njjDRRFwc7OjkWL\nFumf9LrvvvuYPn06Tk5OjB8/nosXLwIwe/Zs5s2bx/jx4+nTp49+VqdXr14NjsvKyoqVK1fy+uuv\nU1ZWhoWFBfPnz0elUjF79myOHDnC2LFj8fLyYuzYsXVmKX7reo3P7/31r3+97Xfg6enJG2+8wdNP\nP01NTQ0+Pj68/vrrDfr+7OzsCAgI4Pz58wQFBQEQHBzMDz/8wIQJE7C0tMTZ2Znly5cD8OKLL+qf\nzGqMgIAAnnzySaKiotDpdLi4uPDaa6/V26a+n8uIiAhiYmKIjIzE1dWVUaNGceTIESwsLJg5cyZ/\n+MMfgGuzeq+88grW1taNilcIY1Mpv71hLoQQzeTIkSO8+OKL7Nq1q6VDEUK0IzLnKIQQQoh2QxIf\nIYQQQrQbcqtLCCGEEO2GzPgIIYQQot2QxEcIIYQQ7Ua7epw9N/fmj68ag5OTDYWF5SbrXxhOxqZ1\nknFpvWRsWi8Zm4Zxc7O/5Wcy42MkarV5S4cgbkHGpnWScWm9ZGxaLxmbppPERwghhBDthiQ+Qggh\nhGg3JPERQgghRLshiY8QQggh2g1JfIQQQgjRbkjiI4QQQoh2QxIfIYQQQrQbkvgIIYQQAoCYmJ0N\nOu79998lMzPjlp+//PLzxgrJ6CTxEUIIIQRZWZns2LGtQcfOn78QLy/vW36+YsV7xgrL6NrVlhVC\nCCGEuLn33nubs2cTGTEimPHjJ5KVlck//rGat976C7m5OVRUVPDoo08QGjqCefOe4PnnX2T37p2U\nlZWSmnqFjIx0nntuIcOGhTJ58hh++GEn8+Y9QXDwEBISjlBUVMTbb/8dV1dX/vKXJVy9mkW/fv3Z\ntWsH33+/tdmuUxIfIYQQopX5ZlcSh8/l3PC+ubkKrVYxqM/gXu7MHt39lp/ff38U0dHf4OfnT2rq\nZVav/g+FhQWEhAxl4sQpZGSks2TJy4SGjqjTLicnm7/9bSWxsQfZuPE7hg0LrfO5ra0t77//ER99\n9AF79+7Cy8uH6uoqPv74Mw4c2Mc33/zPoOsxlCQ+oo6aWh3HLuYysKcbanO5EyqEEO1R794BANjb\nO3D2bCKbNkWjUplRXKy54dj+/YMAcHd3p7S09IbPAwMH6D/XaDRcuZJCv36BAAwbFoq5efPuPyaJ\nj6hjw/5L/BibyuyI7kQO6dLS4QghRLs0e3T3m87OuLnZk5tbYvLzW1hYAPDzzz9RXFzMhx/+h+Li\nYv7v/6JuOPa3iYui3Dgb9fvPFUXBzOzaeyqVCpVKZezw6yX/pBd6BcWV7DiSDsCe4xk3/QEWQgjR\nNpmZmaHVauu8V1RURKdOXpiZmbFnzy5qamqafB5vbx/Onz8DQHx87A3nNDVJfITehv0p1NTqcLLv\nQHZhBeeuFLZ0SEIIIZpJ165+nD9/jrKyX29XhYeP5uDBfcyf/xTW1ta4u7uzZs2/m3Se4cNHUFZW\nxlNPPcaJE8dwcHBsauiNolLa0T/rTTk92FzTj6aSkVvKq5/G4+ViS9SEu1jxZQKDe7nz9D19Wzq0\nJrvTx6atknFpvWRsWq+2MDbFxRoSEo4QHj6G3Nwc5s9/iq+++s6o53Bzs7/lZ1LjIwD4bs8lFAVm\nhPvTw8cRHzdbjl3IRVNWjaOtZUuHJ4QQoo2wsbFl164dfPXVWhRFx7PPNu9ih5L4CC6kFXE8KY+e\nnTsS6O+CSqViVJA3X/58gf0nM5k8zLelQxRCCNFGqNVq/vKXt1rs/FLj084pisK3MUkAzAr311fX\nDwvwxNLCjD3HM9G1n7uhQggh2jhJfNq5YxfzSM4oZlBPN/y9fy0ws7FSM6S3B3maSs6kFLRghEII\nIYTxmPRW1/Llyzlx4gQqlYrFixfTv3//G4559913OX78OGvXriUuLo758+fTo0cPAHr27MmSJUt4\n+eWXSUxMpGPHjgA89thjhIeHs2nTJj7//HPMzMyYPXs2s2bNMuXltDlanY7v9iRjplJx76huN3we\nPsCbfSez2H0sg77dXFogQiGEEMK4TJb4xMfHc+XKFdatW0dycjKLFy9m3bp1dY5JSkri8OHD+oWS\nAEJCQli5cuUN/T3//PNEREToX5eXl/Phhx+yfv16LCwsmDlzJuPGjdMnR+L29p/MIiu/nPAgLzq5\n2N7wua+nPV097DmRlE9hSRVO9h1aIEohhBDCeEx2q+vQoUOMHTsWAH9/fzQazQ1LWa9YsYIFCxYY\n1P+JEyfo168f9vb2WFlZMXDgQBISEpocd3tRVaNlw/4ULC3MuDvM76bHqFQqRg3wQqco7DuR2cwR\nCiGEaI1mzpxKeXk5a9d+xunTJ+t8Vl5ezsyZU+ttHxOzE4CtWzezZ89uk8V5Kyab8cnLyyMgIED/\n2tnZmdzcXOzs7ACIjo4mJCQEb++629onJSXx5JNPotFomDdvHqGh1zY7++9//8uaNWtwcXFhyZIl\n5OXl4ezsfEP/9XFyskGtNt2eIPWtG9DafLPjAprSau4b25Mefq63PG7yCH++3Z3E/tNX+cO0fpib\nNe/S4sZyJ41NeyLj0nrJ2LReLT025uZmuLra8fzzz97wWVmZGebmZreMMT09nX37djFr1j08/PAD\npg71pprtcfbfrpNYVFREdHQ0a9asITs7W/++r68v8+bNY+LEiaSlpfHQQw+xfft2pk2bRseOHend\nuzcff/wxq1atYsCAAbfs/1YKC8uNd0G/cyctKlVSXs36XRews7ZgZD/P28Y9pI8nMccy2BV3maDu\nt06SWqs7aWzaExmX1kvGpvUy5dg8+uhcli9/F09PT65ezWLRooW4ublTUVFBZWUlCxb8iT59+qLV\n6sjLK+XPf36V8PAxBAUN4M9/fpHq6mr69w9Cq9WRm1vC9u0/sn79OszNzfD19eell/7MK6+8ytmz\nifz1r++h0+no2LEjM2bcx+rV73Pq1Alqa7XMmDGbyMjJzJv3BMHBQ0hIOEJRURFvv/13PD09G/w9\n3YrJEh93d3fy8vL0r3NycnBzcwMgNjaWgoIC5s6dS3V1NampqSxfvpzFixczadIkALp06YKrqyvZ\n2dkMGzZM38/o0aNZtmwZEyZMuKH/oKAgU11Om/LDoStUVGm5f0w3rDvc/kcgPMiLmGMZxBzLuCMT\nHyGEuNNEJ23hWM6pG943N1Oh1Rm2xMgA937c233KLT8fOTKCAwf2MmPGbPbt28PIkRH4+/dg5Mhw\njh49zJdffs6bb75zQ7tt236kWzd/nntuITt3bmfHjm0AVFRU8O67H2Bvb88zzzxOcnIS998fRXT0\nNzzyyON88sm/ADh+PIFLl5L56KNPqaio4OGH5zByZDgAtra2vP/+R3z00Qfs3buL2bObPktkshqf\n0NBQtm27dvGJiYm4u7vrb3NFRkaydetWvvnmG1atWkVAQACLFy9m06ZNfPLJJwDk5uaSn5+Ph4cH\nzz77LGlpaQDExcXRo0cPAgMDOXXqFMXFxZSVlZGQkMDgwYNNdTltRl5RBbsS0nF1tCJ8gPftGwBd\nPOzp5uXAqeR88jQVJo5QCCFES7iW+OwDYP/+PYSFjWLPnp089dRjfPTRB2g0mpu2u3z5En37BgIw\nYMAg/fsODg4sWrSQefOe4MqVFDSaopu2P3fuDEFBAwGwtrbG17eb/nd+YOC1uzvu7u431AkbymQz\nPgMHDiQgIIA5c+agUqlYunQp0dHR2NvbM27cuJu2GT16NC+88AI7d+6kpqaGZcuWYWlpydy5c/nj\nH/+ItbU1NjY2vPXWW1hZWbFw4UIee+wxVCoVzzzzDPb2ck/6dr7fd4larcK9I7thoW543jsqyItL\nmcXsPZHFvSNvfPRdCCGE8dzbfcpNZ2dMeaurWzd/8vNzyc6+SklJCfv2xeDq6s6SJa9z7twZVq36\nx03bKQqY/VL/qftlNqqmpob33vsrn332FS4urrz44h9veV6VSsVvq1Vqa2v0/Zmb/1qXa6ytRU1a\n4/PCCy/Ued2rV68bjvHx8WHt2rUA2NnZ8c9//vOGY4YOHcp33924gVlkZCSRkZFGirbtS80uITYx\nmy4edoT08WhU25DeHny9M4l9JzO5O9QXtbmsfSmEEG3NsGFhfPzxakaMGEVRUSH+/tfW1duzZze1\ntbU3bdOlS1fOnTtLePgYEhKOAFBeXoa5uTkuLq5kZ1/l3Lmz1NbWYmlpiVarrdO+V68APv/8E6Ki\n/kB5eTkZGen4+HQx2TXKb692ZH1MMgowK7w7ZqrGPZ3VwcKc4X090ZRWcyIp3zQBCiGEaFGjRkWw\nY8c2wsPHEBk5mXXrvmTBgmcICOhLfn4+P/yw6YY2kZGTSUw8xfz5T5GWdgWVSoWjY0eCg4fwf//3\nEGvW/JsHHohi5cr36NrVj/Pnz7Fy5bv69oGBQdx1Vy+eeeZxFix4hiefnIe1tbXJrlGlGGvu6A5g\nyqcUWvtTEGcuF/C3r4/Tx9eJF+YMuH2Dm8jILWXJJ/EE+Dmz8L47p5C8tY9NeyXj0nrJ2LReMjYN\nU99TXTLj0w7oFIVvY5IBmBnub3A/3m529PBxJDGlgJwiKXIWQghx55HEpx04ci6HK1dLGNLHA19P\nhyb1FR507UmwPcczjBGaEEII0awk8WnjarU6ovdcwtxMxXQjPI01uJcbtlZq9p/MolarM0KEQggh\nRPORxKeN23M8k5yiCsIHeOPesenFYhZqc0L7daKkvIaEC/VvESKEEEK0NpL4tGEVVbVsOpCClaU5\nU0N9jdbvqCAvAGKOye0uIYQQdxZJfNqwbfGplJTXEDmkCw42lkbrt5OLLb26dORcahFZ+WVG61cI\nIYQwNUl82ihNWTXb4tNwsLVkfHBno/d/fbuLPcczjd63EEIIYSqS+LRRmw6kUFWjZVqYH1aWxl+g\ne2BPN+xtLDhwKouaWu3tGwghhBCtgCQ+bVB2QTl7j2fi4WTNiP6dTHIOtbkZYf07UVZZy5FzUuQs\nhBDiziCJTxsUvfcSWp3CjFH+Jt1Ta1TgL0XOsqaPEEKIO4QkPm1MSlYxh8/l4NfJgUF3uZn0XO5O\nNgT4OXMxXUNGbqlJzyWEEEIYgyQ+bYiiKHy7OwmA2RH+qBq5Eakhwq8/2i5FzkIIIe4Akvi0IadT\nCjiXWkR/fxfu6uLULOcM7O6Ko50lB09fpapGipyFEEK0bpL4tBE6ncK3u5NRATNHGb4RaWOpzc0Y\n0d+Liqpa4s9mN9t5hRBCCENI4tNGxJ65SnpuKcP7euLjbtes5x4Z2AkVsqaPEEKI1k8SnzagplbL\n93svoTY3454RTd+ItLFcHa3p5+/CpcxiUrNLmv38QgghRENJ4tMG7ErIIL+4irGDfHBxtGqRGMKD\nrq3kLEXOQgghWjNJfO5w5ZU1bDl4GesOaiYN69picfT3d8HZoQOHEq9SUVXbYnEIIYQQ9ZHE5w63\nNTaVsspaJg/rip21RYvFYWamYmR/L6qqtcRJkbMQQohWShKfO1hhSRU/H0nDyb4DYwf5tHQ4jAj0\nwkylYs8xud0lhBCidZLE5w62cf8lamp13BPmh6WFeUuHg5N9BwK7u3Alu4SUrOKWDkcIIYS4gSQ+\nd6iMvDL2nczCy9WW4f08jdbvhcJkXj34FpeLUw1qHz7glyLnY7J/lxBCiNZHEp87VPSeZBTl2mKF\n5mbGGcZaXS3/O/8d+ZWF7Liyx6A+AvyccXW0Iu5sNuWVUuQshBCidZHE5w50Mb2IYxfz6OHjSGB3\nF6P1G5N+gJzyPABO5CWiqWr8mjxmKhWjgryortFxKPGq0WITQgghjEESnzvMtY1IkwGYFd7daBuR\nFleX8GPKTmzVNkzxG49O0RGXdcSgvsL6dcLcTMWe4xkoimKU+IQQQghjkMTnDnP8Yh5JGRoG9nSj\nu4+j0frdlPwTldpKpnQbzyifUCzMLDiQGYdO0TW6L0e7Dgzo6UZ6bhnJmVLkLIQQovVQm7Lz5cuX\nc+LECVQqFYsXL6Z///43HPPuu+9y/Phx1q5dS1xcHPPnz6dHjx4A9OzZkyVLlpCVlcWiRYuora1F\nrVbzzjvv4ObmRkBAAAMHDtT39dlnn2Fu3vJPN5mKVqdj/Z5kVCqYMcp4W1NcKU4jNusIXraehHoN\nwdzMnEEegcRmHeF8QRK9XXo2us/wIC+OnMsh5lgG3b2Nl6AJIYQQTWGyxCc+Pp4rV66wbt06kpOT\nWbx4MevWratzTFJSEocPH8bC4teF90JCQli5cmWd4/7xj38we/ZsJk2axJdffsmaNWt48cUXsbOz\nY+3ataa6hFbnwKmrZOWXMzLQi04utkbpU1EUvr2wCQWFWT2nYW52LXEM8xpCbNYR9mfGGZT49Orq\nhIeTNYfP5TBnTI8WXVxRCCGEuM5kt7oOHTrE2LFjAfD390ej0VBaWlrnmBUrVrBgwYLb9rV06VIm\nTJgAgJOTE0VFRcYPuJWrqtGyYd8lLNVmTAvzM1q/h7OPkVJ8hQFu/ejp5K9/39ehC952nTjZpCJn\nb2pqdRw8LUXOQgghWgeTJT55eXk4OTnpXzs7O5Obm6t/HR0dTUhICN7e3nXaJSUl8eSTT3L//fdz\n4MABAGxsbDA3N0er1fLVV18xdepUAKqrq1m4cCFz5sxhzZo1prqUVmHHkTSKSqsZF9wZJ/sORumz\nsraKDUlbsTBTM737lDqfqVQqwryGoFN0HMo6bFD/of08UZtLkbMQQojWw6Q1Pr/12198RUVFREdH\ns2bNGrKzf93XydfXl3nz5jFx4kTS0tJ46KGH2L59O5aWlmi1Wl588UWGDh3KsGHDAHjxxRe5++67\nUalUPPjggwwePJh+/frdMgYnJxvUatPVALm52Zuk3+Kyan6KS8XexpKoyQHYGum20Vcnd6KpLmZm\nwGR6delyw+cTHUeyIXkrsdl50giiAAAgAElEQVSHmTt4KmaqxuXJbkBof2/2HEsnp6Savv6uRonb\nEKYaG9E0Mi6tl4xN6yVj0zQmS3zc3d3Jy8vTv87JycHNzQ2A2NhYCgoKmDt3LtXV1aSmprJ8+XIW\nL17MpEmTAOjSpQuurq5kZ2fTuXNnFi1aRNeuXZk3b56+z/vvv1//56FDh3LhwoV6E5/CwnJjX6ae\nm5s9ubmNvyXUEF/vvEhZZS1zxvSgvLSS8tLKJveZW57PlnM7cOrQkVDXYbeMfaB7IIeyDrPvfAJ9\nXO5q9HmG9nZjz7F0NsQk4eFgnJmqxjLl2AjDybi0XjI2rZeMTcPUlxya7FZXaGgo27ZtAyAxMRF3\nd3fs7OwAiIyMZOvWrXzzzTesWrWKgIAAFi9ezKZNm/jkk08AyM3NJT8/Hw8PDzZt2oSFhQXPPfec\nvv9Lly6xcOFCFEWhtraWhIQE/dNgbUmepoJdCem4OFgRMcD79g0aKDppC7WKlundJ2NpbnnL48K8\nhwBwIDPOoPP07NyRTi42HD2fQ3F5tUF9CCGEEMZishmfgQMHEhAQwJw5c1CpVCxdupTo6Gjs7e0Z\nN27cTduMHj2aF154gZ07d1JTU8OyZcuwtLTkq6++oqqqiqioKOBasfSyZcvw9PRk5syZmJmZMXr0\n6Js+Ln+n+35vCrVahXtHdsNCbZw89WzBBU7mJdKjYzcGutf/nXW17/xLkfMZNFXFOHZwaNS5VCoV\n4UHe/G/nRQ6cymLikK5NCV0IIYRoEpXSjqpOTTk9aIrpx9TsEl5bcxgfdzuWPhKMmRFWadbqtCyP\n/zvZ5bm8HDwfH3uv27bZm36IdRe+Z2q3CUT6jmn0Ocsqa3h+1QGc7Duw/ImhRrmOxpCp4dZJxqX1\nkrFpvWRsGqZFbnWJpvtuzyUUYFa4v9GShT0ZB7lankOo95AGJT0AwZ5BWJpZcCAz3qCVnG2tLAjp\n5U5OYQVnrxQ2ur0QQghhLJL4tFJnrxRy6lI+vbs6EeDnbJQ+S6pL2ZryM9Zqa6b6TWhwO2u1NYM9\ngiioLORswUWDzj3ql/qkPccyDGovhBBCGIMkPq3QtY1IkwCYGe5vtI1IN1/6iYraSqb4jcfOsnEr\nP4d5DwXgQEasQef293LAx82OYxfz0JRWGdSHEEII0VSS+LRCR87ncvlqCSG93fHr1Lhi4ltJLUnn\nYOZhOtl6MOKXJKYxutj74GPnxan8sxRVaRrdXqVSET7AC61OYd/JrEa3F0IIIYxBEp9Wplar47s9\nyZibqZg+0jgbkf52P66ZPe7W78fVGCqVijDvX1ZyzjxiUBxD+3hiaWHG3hOZ6HTtpqZeCCFEKyKJ\nTyuz90QmOYUVhAd54+FkY5Q+j2Yf55LmMoFufenlbPhaR4M9BmBpbsnBLMOKnG2s1Azt40GeppLT\nKQUGxyGEEEIYShKfVqSyupZN+1PoYGnO1FBfo/RZpa3m++StqM3U3Pu7/bgay1ptxWD360XOFwzq\nY1TQL0XOx6XIWQghRPOTxKcV2RafRnF5DZEhXXCwvfVqyo2x/cpuiqo0jO08Elfrpj8ddn0l5/0Z\nhq3k7NfJga6e9pxIyqeguOlbbwghhBCNIYlPK6Epq+an+FQcbCyYENLZKH3mVRSwI3UPHTs4Mt53\ntFH67OrQmc723pw2sMgZIDzIC50iRc5CCCGanyQ+rcSWA5epqtZyd5gfVpbG2Unk+6Qt1Opqme4/\niQ717MfVWKFe14ucDxvUfkgfD6wszdl7IhOtrvG1QkIIIYShJPFpBbILy4k5noG7kzUjAxu2mvLt\nnC9I4njuafwdfRnkEWSUPq8L9gjC0tzS4JWcrSzVDAvwpLCkipPJ+UaNTQghhKiPJD6twPd7L6HV\nKcwY5Y/avOlDotVpWX9xEypUzOo5zWgLIF5npbYi2COIwqoizuSfN6iPUUHXErw9xzONGZoQQghR\nL0l8WlhKVjHxZ3Pw62TP4LvcjNLnvsxYMsuuMtwrmM723kbp8/fCvK4tgrg/07Ai5y4e9vh7OXAq\nOZ88TYUxQxNCCCFuSRKfFqQoCutjkgGYGd7dKDMzpdVlbLm0HWu1FVO7RTa5v1vp4uBDF3tvTucZ\nXuQ8KsgbhWtrFwkhhBDNQRKfFpSYUsDZK4X06+ZC765ORulzc8o2KmormOQ3DntLO6P0eSuhXkNQ\nUDiYGW9Q++De7th0ULPvRBa1WilyFkIIYXqS+LQQnaLwbUwyKmDGKONsTZFeksmBjDg8bdwZ5T3c\nKH3WZ7BHEB3MLTmYedigIucOFuYM7+uJpqya4xfzTBChEEIIUZckPi0kLjGbtJxShgZ40sXDvsn9\nKYrCtxc3Nmk/rsa6VuQ8oGlFzgNkJWchhBDNRxKfFlBTqyN67yXU5iqmj/QzSp8JOSdJKkqhn2sf\nerv0NEqfDRF6fSXnzFiD2nu72tLTx5HEy4XkFJYbMzQhhBDiBpL4tIDdxzLIL65k9EAfXB2tm9xf\ntbaa75N+QK0yZ0b3qUaIsOG62PvQxd6H03nnKKwsMqiPX2d9pMhZCCGEaUni08zKK2vZcvAy1h3U\nTBnua5Q+f74SQ2FVEaO7jMTNxsUofTZG2PUi5yzDVnIefJcbdtYW7D+VRU2tFDkLIYQwHUl8mtmP\ncVcorahh0tAu2FlbNLm//IpCfk6NwdHSngldI4wQYeMN8gjCyrwDBzPj0eq0jW5voTYntJ8nJeU1\nJFzINUGEQgghxDWS+DSjwpIqfj6cRkc7S8YONs5GpN8n/0CNrpZp/pOwUlsZpc/GslJ3YLDnAIqq\nNJwpMHQlZylyFkIIYXqS+DSjjftTqK7Vcc+IbnSwaPpTVxcKkzmWcxI/h64Eew4wQoSGC/P6pcg5\nw7CVnD2dbejd1YlzqUVk5ZcZMzQhhBBCTxKfZpKVX8a+k5l0crEhtJ9nk/urux/X3ZipWnYoO9t7\n09W+M4n5TShylv27hBBCmJgkPs3kuz2XUBSYOcofc7Omf+0HMuPJKM1iaKfBdHUwzm2zpgrzbtpK\nzgN7uuFgY8GBU1lU1zS+VkgIIYS4HUl8mkFSuoaEC7l093YkqIdrk/srqylny6VtWJlbcbe/6fbj\naqyB7oHXipyzDhtU5Kw2NyOsvxdllbUcOZ9jggiFEEK0d5L4mJiiKHwbkwTArAh/o2xEuuXSdspq\ny5noNwYHy6av+mwsVuoOBHsOpKhKQ2L+OYP6GPnL7a4Yud0lhBDCBCTxMbETSflcTNcwoIcrPXw6\nNrm/jNIs9mUcwsPGjXCfUCNEaFzXi5wPZBpW5Oze0Zq+fs4kpWtIzy01ZmhCCCGEJD6mpNXpWL8n\nGZUK7h3l3+T+FEVh/YVNKCjM6DEVtZnaCFEal4+9F10dOpOYf56CykKD+tA/2n5MZn2EEEIYl0kT\nn+XLl3PfffcxZ84cTp48edNj3n33XaKiogCIi4tj6NChREVFERUVxeuvvw5AVlYWUVFRPPDAA8yf\nP5/q6moANm3axIwZM5g1axbffvutKS/FIAdPXSUzr4wR/Tvh7Wrb5P6O557mQlEyfV16EeDSywgR\nmkaY19BfipwNW8k5sLsLHe0sOZh4lapqKXIWQghhPCZLfOLj47ly5Qrr1q3jzTff5M0337zhmKSk\nJA4frvvLMSQkhLVr17J27VqWLFkCwMqVK3nggQf46quv6Nq1K+vXr6e8vJwPP/yQzz77jLVr1/L5\n559TVGTYY9SmUF2jZcP+FCzUZkwL69b0/rQ1RCdtwVxlzowezbsfV2MN8gjEytzK4JWc1eZmjOjv\nRUVVLfFns00QoRBCiPbKZInPoUOHGDt2LAD+/v5oNBpKS+vWbKxYsYIFCxbctq+4uDjGjBkDQERE\nBIcOHeLEiRP069cPe3t7rKysGDhwIAkJCca/EAPtOJpOYUkV4wZ3xsm+Q5P725m6h4LKQiI6h+Fu\n42aECE2ng7klIZ4D0FQXc9rQIudAL1QqKXIWQghhXCYrEsnLyyMgIED/2tnZmdzcXOzs7ACIjo4m\nJCQEb2/vOu2SkpJ48skn0Wg0zJs3j9DQUCoqKrC0tATAxcWF3Nxc8vLycHZ2vqH/+jg52aBWN33F\n5Ftxc7v2hFVJeTU/xl7B3saCqCkBTd6TK6+8gO2pu3G0cuDBwdOwsWj6ju6mNkU9mr0Zhzicd5Sx\nfYY2ur2bmz2De3tw+Ew2xVVa/JtYGH59bETrIuPSesnYtF4yNk3TbNWxiqLo/1xUVER0dDRr1qwh\nO/vXWxm+vr7MmzePiRMnkpaWxkMPPcT27dtv2U9D3v+twsJyA6O/PTc3e3JzSwBYt+siZZW13De6\nOxWllVSUVjap709Pf0O1tobZPadTVlRLGSXGCNmkbHHE16ELx7MSOZeaiou1U6P7GNbnWuKzYfdF\nHoo0vKbpt2MjWg8Zl9ZLxqb1krFpmPqSQ5Pd6nJ3dycvL0//OicnBze3a7doYmNjKSgoYO7cucyb\nN4/ExESWL1+Oh4cHkyZNQqVS0aVLF1xdXcnOzsbGxobKymvJQ3Z2Nu7u7jft393d3VSX02B5mgp2\nHk3HxaEDowd6377BbSQVpXA05wRdHTozxHOgESJsPmFev6zknGXYSs79u7ng7NCBQ2eyqaiqNXJ0\nQggh2iOTJT6hoaFs27YNgMTERNzd3fW3uSIjI9m6dSvffPMNq1atIiAggMWLF7Np0yY++eQTAHJz\nc8nPz8fDw4Phw4fr+9q+fTsjRowgMDCQU6dOUVxcTFlZGQkJCQwePNhUl9NgG/elUKtVmD6yGxZN\nvK2mU3R8e2EjALN6TGvx/bgaa5BHINZqKw4ZWORsZqZiZKAXVdVa4s5IkbMQQoimM9mtroEDBxIQ\nEMCcOXNQqVQsXbqU6Oho7O3tGTdu3E3bjB49mhdeeIGdO3dSU1PDsmXLsLS05Nlnn+Wll15i3bp1\neHl5cc8992BhYcHChQt57LHHUKlUPPPMM9jbt+x9z7ScUg6evoqPmx1D+zR9I9KDmfGkl2YyxHMQ\nfo5djBBh87I0tyTYYyB7Mw5yOv8sgW59G93HiP5ebNp/mZjjGYwK8jLKytdCCCHaL5XSkOKYNsKU\n90Xd3Oz58+r9nEzO54+zAunv79Kk/sprynkt9h1qdDUsHfoijh0cjBRp88oozWJ5/N/p43IXzwQ+\nZlAfq6JPkXAhl1ceGkw3r8Z/D3JPvHWScWm9ZGxaLxmbhmmRGp/25lRSHieT8+nVpSP9ujnfvsFt\nbE3ZQWlNGRN9x96xSQ+At10n/By6cDb/AvkVBQb1Ea7fvyvDmKEJIYRohyTxMQJFUfjsh0QAZkV0\nb/LtmMzSq+zJOIibtQvhncOMEWKLCvW+vpKzYUXOffyccXW0Iv5sNuWVNUaOTgghRHsiiY8RHD2f\ny4XUIgb3csevU9NmZxRF4buLm9EpOmb0mIpFK9yPq7EGufe/VuScddiwImeVilFBXlTX6DiUKEXO\nQgghDCeJjxGcTslHbW7GjJFN35riZF4i5wov0sf5Lvq69DZCdC3P0tySEM+BaKpLOJV/1qA+wvp7\nYW6mIuZ4RoPWbBJCCCFuRhIfI5gZ3p0P/xSBh7NNk/qp0dbw3cUtmKnMmNFjapt6ginM69rqzfsz\nYg1q72hrycCebmTklpGUoTFmaEIIIdoRSXyMwM7aAi83uyb3szNtH/mVBYT7hOJp2/KLMRqTl50n\n3Ry7cq7gInlNLXI+Jvt3CSGEMIwkPq1EUZWGbVd2YW9hxyS/sS0djkmEXl/J2cAi515dnfBwtuHw\nuRxKK6TIWQghRONJ4tNKbEjaSrW2mrv9I7FWt/5NSA0x0D0Qa7W1wUXOKpWKUYFe1Gp1HDyVZYII\nhRBCtHWS+LQClzSXOZx9jC723gzt1PLbbpiKpbkFIZ4DKa4u4VTeGYP6CO3nidrcjJjjmVLkLIQQ\notEk8Wlhdfbj6nnn7cfVWGFeQwDYnxlnUHt7G0sG93LjakE551OLjBmaEEKIdqBt/5a9A8RmHSG1\nJINgjwF0c/Rt6XBM7lqRsy9nCy40ocj52q73spKzEEKIxpLEpwVV1FawKfknLM0tuaf7pJYOp9lc\nn/U5YOCsTw8fR7xcbTl6PpfismpjhiaEEKKNk8SnBW1N2UFJTSmRXUfTsYNjS4fTbAa498emqUXO\nQV5odQoHpMhZCCFEI0ji00KuluUQk34AVytnRnce0dLhNCtLcwuGeA6ipLqUkwYWOQ/v64ml2ow9\nxzPRSZGzEEKIBpLEpwUoisL6i5vQKTru7TEVC3OLlg6p2YV6/1LkbOBKzrZWFgT3dienqIKzlwuN\nGZoQQog2TBKfFnA6/yxnCy7Qy6kH/V37tHQ4LaKTrQf+jr6cK7xIXkW+QX1IkbMQQojGksSnmdXo\nall/cTNmKjNm9ry7Te3H1Vhh3tf27zpg4ErO3bwc6Oxux/GLeRSVVhkzNCGEEG2UJD7NbHfaPvIq\n8hnlPZxOth4tHU6LCnLrd63IOfMwtbraRrdXqVSE/1LkvO+kFDkLIYS4PUl8mpGmqpifLu/EzsKW\nSX7jWjqcFmdpbsGQToMoqTG8yHlogCcdLMzZezwTnU6KnIUQQtRPEp9mtDH5R6q01UztNgEbi7a5\nH1dj6df0yTBsTR/rDmqG9PEgv7iS0ymG1QoJIYRoPyTxaSYpmlTirh6ls50Xw71CWjqcVsPT1gN/\nRz/OFV4kpzzPoD7CB3gBEHMs05ihCSGEaIMk8WkGv92Pa2Y72I+rscJ+ebT9oIFFzr6eDvh62nMi\nOY+C4kpjhiaEEKKNkd/AzSDuagJXStIY5B5I945+LR1OqzPArR+2ahtis44YVOQMED7AG0WBvSdk\n1kcIIcStSeJjYhW1lWxM3oqFmQXTu09u6XBaJQsjFDmH9HbHuoM5+05modXpjByhEEKItkISHxP7\n6fJOSqpLmdA1Aierji0dTqsV6tW0lZytLNUMDfCksKSKk0lS5CyEEOLmJPExoezyXHan7cfFyokx\nXUa1dDitmqetO907+nG+MMnwImf9Ss5yu0sIIcTNSeJjQtEXN6NVtNzbfQqW7XA/rsYK87q2krOh\nRc6d3e3w93bg9KV88ooqjBmaEEKINkISHxM5nXeW0/nn6OnUnUC3vi0dzh0hyK0vthY2HMoybCVn\nuDbrowB7pMhZCCHETZg08Vm+fDn33Xcfc+bM4eTJkzc95t133yUqKqrOe5WVlYwdO5bo6GgAnnvu\nOaKiooiKimLq1KksWbKE9PR0BgwYoH//ueeeM+WlNEqtrpbvkq7txzWrR/vej6sxLMwtGOI5iNKa\nMk7kJhrUR3Avd2w6qNl3MotarRQ5CyGEqEttqo7j4+O5cuUK69atIzk5mcWLF7Nu3bo6xyQlJXH4\n8GEsLOreBvroo49wdHTUv165cqX+z4sWLWLWrFkA+Pn5sXbtWlNdgsFi0g+QU57HKJ/heNl5tnQ4\nd5QwryHsStvH/sw4BnkENrq9pYU5w/t5suNIOscv5jG4l7sJohRCCHGnMtmMz6FDhxg7diwA/v7+\naDQaSktL6xyzYsUKFixYUOe95ORkkpKSCA8Pv6HPS5cuUVJSQv/+/U0VdpNpqkr4MWUHthY2TPYb\n39Lh3HE8bN3p0bEbFwqTyCnPNaiPX4ucM4wZmhBCiDbAZIlPXl4eTk5O+tfOzs7k5v76iyw6OpqQ\nkBC8vb3rtHv77bd5+eWXb9rnF198wYMPPljnHM899xxz5sxh06ZNRr4Cw2y69COV2iqm+E3A1sKm\npcO5I+n37zKwyNnL1ZaenTty5nIh2YXlxgxNCCHEHc5kt7p+T1F+3Tm7qKiI6Oho1qxZQ3Z2tv79\nDRs2EBQUROfOnW9oX11dzdGjR1m2bBkAHTt2ZP78+dx9992UlJQwa9Yshg4dirv7rW9tODnZoFab\nG++ifkdjlk9s1hG6OnozPXAsZmZSO26Isc7DWJ+0ibjsozwSMgMLA56ImzrSn3e/PMrh83n07emB\nm5u9CSIVTSXj0nrJ2LReMjZNY7LEx93dnby8X9djycnJwc3NDYDY2FgKCgqYO3cu1dXVpKamsnz5\ncnJyckhLSyMmJoarV69iaWmJp6cnw4cP5/Dhw3VucdnZ2TFjxgzg2mxS3759uXTpUr2JT6EJ//Xv\n4mrLv+P/B8D0blPIzy8z2bnagxCPQexM28uOs7EM9ghqdPueneyxs7Zge9wVHpzYiyKZ+Wl13Nzs\nyc0taekwxE3I2LReMjYNU19yaLLEJzQ0lA8++IA5c+aQmJiIu7s7dnZ2AERGRhIZGQlAeno6ixYt\nYvHixXXaf/DBB3h7ezN8+HAATp06Ra9evfSfx8bGsnv3bhYtWkR5eTnnzp3Dz6/l9sHaf+UwKcWp\nDHDvTw8n/xaLo60I9R7CzrS9HMiIMyjxsVCbEdavEz/Fp3LgZBYBnR1v30gIIUSbZ7J7MQMHDiQg\nIIA5c+bwxhtvsHTpUqKjo/n5558N6i83NxcXFxf968GDB6PRaLjvvvt46KGHeOKJJ/Dw8DBW+I1S\nWVvJlye+x8JMzXR/2Y/LGDxs3OjZ0Z8LRclkG1jkPCrICxWwev0JDp2+atwAhRBC3JFUym+Lb9o4\nU00Pbk35mR9SfmaS71gmd5MnuYzlSPZx1iR+xZguI7m3+xSD+og9c5W12y5QUVXL0D4ePDj+Lmys\nmq20TdRDpuxbLxmb1kvGpmHqu9Ul1bdGYK22pq/7XYzrGt7SobQpgW59sbOwJS7rKDUGruQ8tI8n\nKxeG083Lgdgz2SxbE09SusbIkQohhLhTSOJjBBGdw3g14o9Ymlu2dChtioWZmiGdflnJOeeUwf14\nutjy8tyBTBnuS35xJW99eZQN+y6h1cnKzkII0d5I4iNatetr+uzPjGtSP2pzM+4d2Y2XHhiIs30H\nNh24zNtfHpPNTIUQop2RxEe0au42bvR06s7Foktkl+U0ub+enTvy2qMhhPR2JylDw9I18cQmSuGz\nEEK0F5L4iFYvzCsEaPqsz3U2Vhb8v7sDeGxyb3Q6+HjzGf69OZGKKsPqiIQQQtw5DE58Ll++bMQw\nhLg1fZHz1aPUaGuM0qdKpSK0XyeWPRqMXyd7DiVms/TTeJIypPBZNE52QTkr159k++E02tFDskLc\nsepNfB555JE6r1evXq3/86uvvmqaiIT4HbWZmmGdgimrKed47mmj9u3hZMOiBwcxeVhX8jWVrPhv\nApsOpKDTyS8wcXuHEq+y7LPDHE/K4+udF/nq54vysyNEK1dv4lNbW3fqPzY2Vv9n+ZeNaE7Df7nd\ndcBIt7t+S21uxoxR/rz4wAAc7SzZsC+Ft79KIE8jhc/i5qqqtXz6w1n+vfkMAA+O74m3my07E9JZ\nveE01TXaFo5QCHEr9SY+KpWqzuvfJju//0wIU3K3ceWuX4qcrxqhyPlm7urixF8eC2HwXW5cTNew\n9NPDxJ3Jvn1D0a6k5ZTyl88Ps/9UFl097Fn2SDCjB/qwaO5AenXpSMKFXP627jilFca5LSuEMK5G\n1fhIsiNaUugvj7abYtbnOlsrC566py+PTOyFTqfwr02JfLLljBQ+CxRFYXdCOq9/foSs/HLGB3dm\ncdQgPJxsgGtF8wtmB117YjBdw1v/PSqzhkK0QvWu3a/RaDh06JD+dXFxMbGxsSiKQnFxscmDE+K3\nAt0CsLewIy7rKHd3i8TC3MIk51GpVIwI9KJn5478a1MiB05f5WK6hsfv7oO/l2x22h6VV9aw5sdz\nHD2fi62Vmqen9yWou+sNx1mozXji7gCc7DuwLT6NN9ceZcGsQLp43Hr5fCFE86p3r66oqKh6G69d\nu9boAZmSKfc3kf1TmseGpK38nBrDw33mEOI5sEFtmjI2tVodG/al8GPsFVQqFdNG+DF5aFfMzGT2\ns6nulL8zyRka/rkxkfziSnr6OPLE3QE4O1jdtt32w2ms23mRDpbmPHNvPwJ8nZshWuO4U8amPZKx\naZj69uqSTUqNRH4Ym0dOeR6vxf4Vf0c/nh/0VIPaGGNszl4p5D9bzlBYUkXPzh15fEofXBxv/8tP\n3Fpr/zujUxR+iksles8lFEVhaqgvU0N9MTdreIVA/Nls/rPlDIoCj07qzbC+niaM2Hha+9i0ZzI2\nDWPwJqWlpaV89tln+tdff/0106ZN47nnniMvL89oAQrRUO42rvRy6kGyJoWrZc1XeNy7qxOvPRrC\noJ5uXEgrYumn8cSflcLntkpTVs3fvznB+phkHGwt+NP9A7hnRLdGJT0AIb09WHhfEJYW5vx7yxm2\nxl6RJ2KFaGH1/i1+9dVXyc/PByAlJYX33nuPl156ieHDh/Pmm282S4BC/F6ot3H272osO2sLnp7e\nlz9M7EWtTsc/Nyby6Q9nqayWwue2JPFyAUs/jScxpYD+/i4sezSEXl2dDO7vri5OLHpwIE72HVgf\nkyxr/QjRwupNfNLS0li4cCEA27ZtIzIykuHDhzNnzhyZ8REtpr9rH32Rs7FWcm4olUrFyEAvlv4h\nmK4e9uw/lcWyNYdJyZJi/zudVqfjuz3JvPf1ccoqarhvdHeem9kfBxvLJvft42bHn6MGyVo/QrQC\n9SY+NjY2+j/Hx8czdOhQ/Wt5tF20FLWZmmFewZTXVnAs91SLxNDJxZY/PzSIiUO6kFtYwfK1R/nh\n0GX5l/wdKl9TydtfHuOHQ1dw7WjF4qhBTAjpgpkR/z/n7GAla/0I0QrUm/hotVry8/NJTU3l2LFj\nhIaGAlBWVkZFhaxPIVpO6PWNSzNib3Ok6ajNzZgV0Z2Fc4Kwt7Hguz2XeOd/xygormyxmETjHT2f\nq9+nLaS3O0v/EIJfJweTnEvW+hGi5dWb+Dz++ONMmjSJqVOn8vTTT+Po6EhlZSUPPPAA99xzT3PF\nKMQNXK1dfilyvkxWMxY530wfX2f+8tgQBvRw5fwvhc9HzplmdWlhPDW1Wv67/Twffn+KWq2OP0zs\nxf+7OwAbq3qXN2uy67YOTjYAACAASURBVGv9TAjpTFZ+OW+uPUpqtjylI0Rzue3j7DU1NVRVVWFn\nZ6d/b//+/YSFhZk8OGOTx9nblmM5p/jP6bVE+IQxs+fdtzyuucZGURT2nMjk6x0Xqa7VMaJ/J+4f\n2wMrS9P+Ir1TteTfmaz8Mv61MZHUnFK8XW15cloA3m52t29oZL9d62fevf3o00rW+pH/n7VeMjYN\nU9/j7PX+HzkzM1P/59+u1NytWzcyMzPx8vIyQnhCGKa/ax/sLe2IvXqUu/0nYmmilZwbSqVSER7k\nzV2/rPi872QWF9KKeOLuAJPdOhGNd+BUFv/dfoGqGi2jgryYM6YHHSzMWySW8cGd6WhnyX+2nOHv\n35zg0cm9GRZwZ6z1I8Sdqt7EZ/To0fj5+eHm5gbcuEnpF198YdrohKiHuZk5wzoFs/3Kbo7lnGRI\np0EtHRLwS+Fz1GCi9yazLT6N5WuPcu/IbkwYYtxiWdE4ldW1rN12gUOJV7HuYM6T0wII6e3R0mER\n0tsDR1tLVn53in9vPkNRSRWRQ7rIAyRCmEi9t7o2btzIxo0bKSsrY/LkyUyZMgVn59YxFWsIudXV\n9uRVFLD00Aq6OfqycNDTNz2mJccmMaWA//xwBk1pNb27OvF/U/rgZN+hRWJpbZpzXFKzS/hoYyLZ\nBeX4dbLn/03ri3tH62Y5d0Ol55by929OUFhSxZhBPtw/pkeLbY0i/z9rvWRsGqa+W13my5YtW3ar\nD3v16sW0adMICwvj5MmTvPXWW8TExKBSqf5/e/cdHlWZ/n/8PZM+ySSZdFIhBVIhhIAUCwgoqwgK\nUqRY19XFyroq4q74/X1XVnT9riuwYAXEFkoWcMWOWCmBQEgFEiCUkDKppCcz8/sjMRLB0DI5M5n7\ndV1eZNrJPd45k0/O85znEBYWhr29dc1dqK9vNtu2XV2dzLp9cX4aBxeOVhdyuKqAwb4JaB3Pnaeh\nZG/8dC6Mig+guLyerKMV/Jh5Gn+dhkAfV0XqsSQ90ReTycS29FOs2JTFmfoWJgwL5Q+T4tB2w9o8\n3c3d1ZGh0X5kH6sgI7+cU2V1JEb6YGd3aatFdwf5PLNc0puL4+r6239gXvK1utavX88//vEPDAYD\ne/bsueLiepIc8emd9pdm8mbWWkYHj2Ja/8nnPG4JvTGZTGzfX0TK120Tn68dFMgdY6NwclRmbokl\nMHdfahtaWLU1l32H9bi5OPD7iTEMjDj3iuqWpr6xhaUbMzl4ooqoYA8emToQN5eenb9mCfuMOD/p\nzcW57Gt1/aympob33nuPKVOm8N577/HAAw+wdevWbitQiCuR4BOLu6OWXcXpNPfwSs4XS6VSMWZw\nEH+9eyghfm58l1HE/6xOo7BYPsDM4fDJKp5ftZt9h/VEh3ryP/cOs4rQA21r/fxpRiJDo/04LGv9\nCNHtujzi88MPP7Bx40aysrK44YYbmDx5Mv379+/J+rqVHPHpvbYUfMbnhdu4M2bGOZOcLa03La1t\nl0b4Iu0EdmoVU6+L4IZhITY38dkcfTEaTWzdWcim749iwsTkq/sxcURfxebKXAmjycS6bfl8kXYC\nDzdH5k8bRKj/b/8V250sbZ8Rv5DeXJyujvh0GXyio6Pp27cvgwYNQn2eqxL//e9/754Ke4gEn95L\n31DB8zuW0M8jlCeGPNTpMUvtTdaRct76JJeaumZi++q472bbmvjc3X2prm3ijY9zyC2sRKd14g+3\nxDIg9PIvLmopvth9nI+25ePcg2v9WOo+I6Q3F+uy1/H5+XT1yspKdLrOHyAnT5684DdevHgxGRkZ\nqFQqFi5cyMCBA895ziuvvML+/ftZu3Ztx32NjY1MnDiRefPmMWXKFBYsWEB2djaenp4A3HfffYwe\nPZotW7awZs0a1Go106dPZ9q0aResSfROPi5eRHtFkVtxiKLaYgLdLH8tlPhwb/7ffcNY9UkuGQXl\nLHpnN/f8LprB/X2VLs3qZB0p563/5lBT30JipA/33hzT4/NizOWGYaF4ap061vq57+YYhstaP0Jc\nti6Dj1qtZv78+TQ1NeHl5cXrr79OWFgY7733Hm+88QZTpkz5zdfu3r2bwsJCUlJSKCgoYOHChaSk\npHR6Tn5+PmlpaTg4dP6AWrFiBR4eHp3u+9Of/sSYMWM6btfX17N8+XI2bNiAg4MDt99+O+PHj+8I\nR8L2XB00nNyKQ/xQtIvp55nkbIncNY48evtAtqWfYt03+SxNzWR0YiAzFFxUz5q0Goz857sjfLrr\nOHZqFXeMjWJccnCvWwNnWIw/7hpHlqZm8sbHOVTWNjFhmKz1I8Tl6HJy8z//+U9Wr17N7t27efLJ\nJ3nuueeYO3cuO3fuZP369V1ueMeOHYwbNw6AiIgIqqurqa2t7fScF198kfnz53e6r6CggPz8fEaP\nHt3l9jMyMkhISECr1eLs7ExSUhLp6eldvkb0bgneMXg4atldvJdmg/Wc7qlSqRg7JJjn7kom2NeV\n7fuL+H+r0+T6TRegr2pgyfvpfLrrOH46F569cwjjh4b02jAQHabjmTlJ6LROrP+mgA++OozReEkn\n5QohuEDwUavVREREADB27FhOnTrFnXfeybJly/D373rFU71e32l4zMvLi7Kyso7bqampDBs2jKCg\noE6vW7JkCQsWLDhne++99x533nkn8+fPp6KiAr1e32kxxV9vX9ien1dybmhtJL30gNLlXLIgXzf+\nelcy45KDOV1ez9/e3cPnu49jvLQVJ2zCnrxSFq1Ko6CohuGx/iy6eyh9A3r/ZUGCfd14du4Qgnxd\n+XrvSVZszqKl1aB0WUJYlS6Hun79l1OfPn0YP378ZX2js+dQV1VVkZqayqpVqygp+eXK2ps2bSIx\nMZGQkJBOr508eTKenp7ExMTwxhtvsGzZMgYPHvyb2/8tOp0Ge3vzDR90NZlK9IxbNNfzeeE37Crd\nwy0DfxkatabePHbHEEYlBvOvj/aRsi2fQyerefyOJLzcnZUurdtdal+aWgy8vTmLT3ccw8nRjsdm\nJDJ2qG0N+fj6avnHY9fxwqpd7D1YxmstWTx7z7BuX5TRmvYZWyO9uTKXtPTypXy4+Pn5odfrO26X\nlpZ2XPNr586dVFRUMHv2bJqbmzl+/DiLFy+mtLSUEydOsH37doqLi3F0dCQgIICRI0d2bOf666/n\n+eef58Ybbzxn+4mJiV3WVFlZf9H1XyqZaW8pHInx7k9O+UH2Hz1EkFsfq+xNmI+GRfcMbVuA71AZ\nD720jXtviiExyjrWorkYl9qXIn0dKzdncbKsjmBfVx6cHE+gjyt6fe2FX9wLPXJbAm/9N4e0vFKe\nePVb/jQ9EW+P7gnH1rjP2ArpzcW57LO69u3b12muTXl5OaNHj8ZkMqFSqdi+fftvvnbUqFEsXbqU\nmTNnkp2djZ+fH25ubZcTmDBhAhMmTADazg575plnWLhwYafXL126lKCgIEaOHMkjjzzCU089RUhI\nCLt27SIqKopBgwbxl7/8hZqaGuzs7EhPTz9nG8I2XR14FTnlB/mxaBfT+9+qdDmXzcPVkcduH8jX\ne0+y7psCXtt4gKHRfkQGexDk40qQjyvuro69/miHyWTih8zTvP/lIZpbjIwZHMSM6yNxtPHJ3w72\nah6YHIdO68QXaSd4Ye0eHu/BtX6EsFZdBp/PPvvssjeclJREXFwcM2fORKVSsWjRIlJTU9FqtZc8\nXDZ79mwef/xxXFxc0Gg0/P3vf8fZ2ZknnniC++67D5VKxUMPPYRWKzu8gHjvGDwc3dldnM6tETcp\nXc4VUalUjEsOITpUx+sfZ5OWV0paXmnH467O9gT5uBLo69b271mBqDdoaGpl7ecH2ZlTgouTPfNu\njSU52k/psiyGWqVi5tgodFonUrbl8+L76T221o8Q1uqSr9VlzWQBQ9vx3yOf8+mxr5kTPY1Jg67v\nFb0xGI2cKqujSF/HKf0v/5ZVNvDrndjNxaE9ELUFoUDvtq/dLejinBfaZ44V17ByczallQ1EBLrz\nwKQ4fCzsiuqWZFdOCW9/koPJxBWv9SOfZ5ZLenNxLnuoSwhrNaLPMD47to0fi3YxadD1SpfTLezU\nakL9tecMZTS3GDhdXk9ReXsYag9Hh05UcfBEVafnajUOnY4MBfq4EuTrZlGL/ZlMJr7cc5L13+Rj\nMJr43fBQbrsmHHsFrlJuTa6K9cfDVdb6EeJCJPiIXsnbRUes9wCyy/MorDqJBo8Lv8hKOTrYERag\nJSygcyBqajFQXF7PKX1t2xGisrYjRHnHq8g73jkQubs6nicQueLq3LOBqLahhXc+yWV/vh53jQO/\nvyWW+H7ePVqDNYsO0/HM7CT+uT6D9d8UUFnTxMyxUVZ5rTIhzEWGurqJHH60PBll2byRuYbBfeKZ\n238mTnaWM8yjpKZmwy9Hh9qHzIr0deirG895rofbeQKRjyuabghEv95nDh6vbDtScaaJmDAdf7gl\nFg8327l2WXeqqGnkn+syOKWvI3mAL/ffEovDJSzlIZ9nlkt6c3Eu+yKlvY0EH9tiMBr41743KKg+\nSqBrAL9PmIu/Rq6D9Vsam1s5XV7/q3lEtZTXNJ3zXM+OQORGkG9bIAr0dkXjfPEHkX/eZ4xGE//9\n6RibfzyKChW3XtOPm4aHyVGKK1Tf2MLSjZkcPFFF/2APHrl94EUfwZPPM8slvbk4EnzaSfCxPa3G\nVj49+QWf5W/H2c6JuTHTSfRLULosq9LQ1Np2hKjsrCNE5XVUnCcQ6bROnY8Q+bYFIhencwORr6+W\nQ0f0vPlxNnnHq/Byd+KBSXFEBcv19rpLS6uxY62fQB9X5k8bdFFr/cjnmeWS3lwcCT7tJPjYJl9f\nLZ9kfsuHeRtpNrYwLvQ6JoVPwE5t2+vAXKmGptZzzjAr0tdReebcQOTt7tR2dKg9FAX6uIKdmlc/\n2kdtQwuDo3y456bec0V1S2I0mVi3LZ8v0k7g6ebI/OmJhPi5dfka+TyzXNKbiyPBp50EH9v0c2+K\naot5M+tdSuv1RHmGc0/cbDycZO2n7lbf2EKR/qxJ1e2hqLr23AvH2tupmHF9FNcnBcnZR2b2+e7j\npGzLx8XJjodvSyCmi7V+5PPMcklvLo4En3YSfGzT2b1paG3kvdx17C/LwsNRy33xc4nw7KtsgTai\nrrHllyNDZXW0mmD0oD6y0nAP6rTWz8QYhseef60f+TyzXNKbiyPBp50EH9v0696YTCa+PvEdmws+\nBeC2yJsZE3y1HHHoYbLPKCOvsJKlqZk0NLUyfUwkNw4LOednX3pjuaQ3F6er4CMrggmbo1KpGBd6\nHY8m3o+rg4aNhz/mnez3aWw993RuIXqbn9f60WmdWPdNPh9+dRij0Wb+/hVCgo+wXVG6CBYMfYwI\nj76klx7g5T3LKK4rUbosIcwu2M+NZ+cOIcjHla/2nmTl5ixaWg1KlyVEj5DgI2yap5MHjw1+gOtD\nrqG4vpSX9ixlb0mG0mUJYXZe7s48MyeJ/iGe7DlYxisf7aeusUXpsoQwOwk+wubZqe2YGnUL98XP\nAeCd7PfZcHgLBqP8BSx6N42zA0/MGERytB+HTlbz9/fSKT/PCt5C9CYSfIRol+Q3kKeSHyFA48c3\nJ37g1X2vU9VUrXRZQpiVg70dD06OY3xyCEX6Ol5Yu4ejRfJzL3ovCT5CnCXA1Z8nkx9hiN8gjlQf\n48Xd/+JQZYHSZQlhVmqVijvGRTHj+kiqapt5etkPZB0tV7osIcxCgo8Qv+Js78Q9cbO4PWoSda31\nLN3/Jl8WbseGVn4QNurGYaE8ODmOVoORV9cd4LuMIqVLEqLbSfAR4jxUKhVjQq7m8cEPonVwY1PB\nVt7MWktDa4PSpQlhVsNi/PnbgyPRONuz+tM8Nn5bgFFCv+hFJPgI0YUIz74sGPYYUZ7hZJRl8VLa\nUopqi5UuSwiziu3nzbNzh+Cvc+GTHYW8sSVbTncXvYYEHyEuwN1RyyOJ9zM+dDSlDXpe3rOU3cXp\nSpclhFn5e2l49s5kooI92J1byssf7udM/bnXWxPC2kjwEeIi2KntuDXyJu5PuBO1Ss2anI9IObiJ\nVmOr0qUJYTZuLg78eWYiV8X6k3+qmhfW7qWkol7psoS4IhJ8hLgEib7xPDX0UQJdA/ju1E/8M30l\nlY1VSpclhNk42Ntx/y2xTBwZRmllA397dw+HTsjPvLBeEnyEuET+Gl/+nPwwQ/0Hc6zmOC+m/Yu8\nisNKlyWE2ahVKqZcG8E9v4umsdnAPz7ax85smesmrJMEHyEug5OdI3fFzmRG/1tpaG1k2f63+OzY\nNowmo9KlCWE21wwK5PHpg3CwV/PGxzl8/NMxWeZBWB0JPkJcJpVKxbXBI5mf9Ec8nNz5+MhnvJG5\nhvoWOeXdlphMJo5UF1LbXKd0KT0irq8XC+cMwdvdif98d4RVW/NoNUjgF9ZDgo8QV6ifRygLhj5G\ntC6KTH0uS9L+xYkzsvCbLSitL+PfGe/wyt7lvJj2L4rrSpQuqUcE+brxlzuT6Rug5YfM0/xzXQb1\ncoFTYSXsnn/++eeVLqKn1JvxVExXVyezbl9cvp7ojZOdI0MDBmMyGTlQnsOu4j14OnkQrA006/e1\nZta8zzQbmtl69EvW5HxESUMZwW6BlDaUsadkP5Ge/dA5eypd4hW5mN44O9ozPDaAIn0dmUcq2J9f\nzsAIbzTODj1UpW2y5v2mJ7m6Ov3mYxJ8uon8MFqunuqNSqVigFckodogMvW57C3NoLqphmhdFHZq\nO7N/f2tjjfuMyWTigD6blQdWk1mei7uTO7NjpjEtahJeLl7sK8skrTidQNcA/F39lC73sl1sb+zt\n1AyN9qOhyUBGvp5duaUMCPVEp/3tXzriyljjfqMECT7tJPjYpp7ujb/GlyS/geRXHSW7PI+cikPE\neEWhcXDpsRqsgbXtM6X1etbkfsRnx7bRbGhhfNho7o2bTYg2CJVKRYg2kDD3YPaVHiCtZB/ujlpC\n3YOVLvuyXEpvVCoVCeHeuLk4sPdgKTuyignycaWPt6uZq7RN1rbfKEWCTzsJPrZJid5oHDRcFTCE\n6uYassvz2F2cTqBbH/w0Pj1ahyWzln2m2dDMp8e+YnX2h5TUlxGti+LBgXeT7J+Ivdq+03P9ND5E\ne0WRUZZNeukBAKI8w1GpVEqUftkupzfhge6E+WvZc6iUndkluDjZEx7obnXv3dJZy36jtK6Cj8pk\nxnMRFy9eTEZGBiqVioULFzJw4MBznvPKK6+wf/9+1q5d23FfY2MjEydOZN68eUyZMoXTp0/zzDPP\n0Nrair29PS+//DK+vr7ExcWRlJTU8brVq1djZ/fbQwplZWe69w2exddXa9bti8unZG9MJhM/Fe1m\n3aFNGExGftdvHL/rOxa1Ss4rsPR9pm1YK4cNh7dQ0ViJp5MHU6NuYbBvwgV/mZfUl7F8/9uUN1Yw\nKnAYM/rfZlXDnVfSm8LiM7y6IYPq2mbGJgUzc1wkdmr5ee8ulr7fWApfX+1vPmb/m49cod27d1NY\nWEhKSgoFBQUsXLiQlJSUTs/Jz88nLS0NB4fOk+FWrFiBh4dHx+1XX32V6dOnc9NNN/H++++zatUq\nnnrqKdzc3DoFJiEsjUqlYlTQVQRrA3kr6z22Hv2SY9XHuStuJm4OMhRgqcrqy1l/eDPZ5XmoVWrG\nh45mQt+xONtf3NwVf40vTwx5iBUZb/Nj0W5qmmu5N24WjnaOZq5ceWEBWv4yN5l/bcjg6/ST6Ksb\neGByHM6OZvt1I8QlMVsM37FjB+PGjQMgIiKC6upqamtrOz3nxRdfZP78+Z3uKygoID8/n9GjR3fc\nt2jRIm688UYAdDodVVWyXLqwLmHuITw99FFivQaQU3GQJWmvcbzmpNJliV9pNrTw3yNf8Lfdr5Bd\nnscAXSTPDpvPrZE3XXTo+ZmHk5bHkh5kgC6STH0OS/e/SV2LbVznytvDmWfmDCGunxcZBeW8+H46\nlWealC5LCMCMR3z0ej1xcXEdt728vCgrK8PNzQ2A1NRUhg0bRlBQUKfXLVmyhL/+9a9s2rSp4z6N\nRgOAwWDggw8+4KGHHgKgubmZJ554glOnTnHjjTdyzz33dFmTTqfB3t58h5u7OrQmlGUJvfFFy3N9\nHmVj9lY2ZG/llfR/c2/SDMaGj7LZeRCW0BdoG9baW3SAVfvWU1ZXjpeLJ3cm3s6IkKQr7I2WRX6P\n8e/d7/LD8TT+lbGSZ699BB9Xr26r3Vy6ozd/++MoVqYe4POdhfz9vb0sun8Effu4d0N1ts1S9htr\n1WPHHs+eSlRVVUVqaiqrVq2ipOSXBb82bdpEYmIiISEh57zeYDDw1FNPMXz4cEaMGAHAU089xaRJ\nk1CpVMyZM4fk5GQSEhJ+s4bKSvP9tSXjrpbL0noz2v86fO0DWJP9IW/seZ8Dpw4yo/9tONrZ1von\nltKXsvpyNhzeTNZ5hrX0+toLb+AizIiYipPJha9PfMczXyzhocT7CHLr0y3bNofu7M3068LROtuz\nYXsBT772HfNujSc+3Ltbtm2LLGW/sXSKzPHx8/NDr9d33C4tLcXX1xeAnTt3UlFRwezZs2lubub4\n8eMsXryY0tJSTpw4wfbt2ykuLsbR0ZGAgABGjhzJM888Q1hYGA8//HDHNu+4446Or4cPH86hQ4e6\nDD5CWIo47wE8PfRR3spay87Tezh5poj7E+bi4yK/EHpKs6GFLwq/4cvj22k1ttJfF8mM/pMJcPXv\n9u+lVqmZEjURdyct/8n/hH+mr+CBhLuI0kV0+/eyNCqVipuGh+Hj4cxb/83l1fUHmHNjf0YnBl34\nxUKYgdmCz6hRo1i6dCkzZ84kOzsbPz+/jmGuCRMmMGHCBABOnjzJM888w8KFCzu9funSpQQFBTFy\n5Ei2bNmCg4MDjz76aMfjR44cYfny5fzjH//AYDCQnp7esU0hrIG3ixd/SprH+sNb+LFoFy+mvcZd\nsTNI8IlVurReL1Ofw/pDWyhvrMDD0Z2pURNJ8htk9iHHcaHX4eHoztrcdSzb/xZ3xd1Bkt+5Z7v2\nRsNi/PHSOvPaxgO8+9lByqoamHpdBGobHeYVyjFb8ElKSiIuLo6ZM2eiUqlYtGgRqampaLVaxo8f\nf0nb+uCDD2hqamLu3LlA22Tp559/noCAAG6//XbUajXXX3/9eU+XF8KSOdg5MCt6Kv08wkg5mMrK\nA6uZEHY9N4ffIKe8m4G+oZz1h7aQVZ6LWqVmXOh1/K7vWJztnXushqEBg9E6uvFG5hreyXqfmv5n\nGB08qse+v5Iigz149s4hvLr+AJ/uPE5ZVSO/vzkGRwfrOdVfWD+zruNjaWQdH9tkLb05caaItzLf\nRd9YQbQuirvj7kDr6KZ0WWbTk31pNrTw5fHtfFH4TduwlmcE0wfcSh8zDGtdrBNnTrE8423ONNdy\nQ9gYJoVPsJhJ7ubuTW1DC8s2HuDQyWoigtx5ZOpA3DW9/1T/7mAtn2dK62qOj6zc3E1kNU3LZS29\n8XDSclVAEqfrSsipOMjekgz6eYShc/a48IutUE/1JVOfw8oDqzmgz0br4Mqs6Nu5LfJmxUOlh5M7\nib7x5JQf5IA+h/LGSuK9YyziSJ+5e+PoYMdVsQHoqxrIPFLB3oOlxPfzQivh54Ks5fNMaXLJinYS\nfGyTNfXGwc6BIf6DsFfbc0Cfza7ivbg6aAjVBlvM0YDuYu6+6BsqeDf3I7Ye/YomQxPXh1zD7xPm\nEupuOf8vNQ4ahvgP4nDlEbIr8ig8c5KBPnHnXAqjp/XEPmOnVpHU3xeTCfYd1rMrp4SIQHd8POSa\ndl2xps8zJUnwaSfBxzZZW29UKhWRnv0I9+hLVnku+8oyKWuoINa7v1Vd9uBCzNWXFkMLnxduY1X2\nB5yuKyXKM5wHB97NsD5JigeK83GycyQ5YDAnak+RU36QgxX5DPSNw0nBVZ57ap9RqVTEhOnwdndm\n78EydmQX4+PpQohf7x3ivVLW9nmmFAk+7ST42CZr7Y2PizfJ/okcqS4kpyKPTH0OA3QRuDn2jktd\nmKMvWfpcVh5YRcbPw1oDpnJbZNtp5JbMXm3HEL9BVDZWk12RR0ZZFvE+MWgcNIrU09P7TKi/lqgg\nD/Yeajvyo1JB/xBPizkyZ0ms9fOsp0nwaSfBxzZZc29c7J25KiCJ+pYGsspz+fbUT+wrPcDpuhKa\nDM24ObriZHdpl1KwFN3Zl7ZhrRQ+OfoljYYmxoRcze8T5hLmHmI1vzzVKjUDfWIxmIwc0OewtzSD\n/roIPJx6fqVjJfYZX08XEqN8OJBfzr7DesprGhkY4Y1abR396ynW/HnWkxS7OrulkbO6bFNv6U16\n6QF+PLWLI9XHaDa2dNzvr/El0jOcKM9wonTheDpZx2To7uhLi6GFr45/y+eF22gxthLlGc70/rcS\n6BbQTVUq49uTP7H+0GYc7Ry4P+FOYrz69+j3V3Kfqa5r5rUNGRw9fYaYMB0P3RaPxtm2VjXvSm/5\nPDO3rs7qkuDTTeSH0XL1tt60Gls5fuYU+ZVHOFx1hILqozQZfvkL0MfFuy0EeYYT6RmOt4tOwWp/\n25X2JUufy/rDW9A3lOPuqGVK5ESS/ROt5gjPhaSXHmBN9ocYMTE3ZjrDApJ67Hsrvc80tRh48+Mc\n0g+V0cdbw/xpg/DxlEnPoHxvrIUEn3YSfGxTb++NwWjgZG0RhyoLyK86Qn7VMRoNjR2PeznrOkJQ\nlGc4Pi5eFhEOLrcv5Q0VbDj8MQf02ahVakYHj+KmfuNx6cFFCHvK4coCXs9cQ0NrI7dF3sy40Ot6\n5Ptawj5jNJpY900+X6SdwF3jwKO3DyI8UC5wagm9sQYSfNpJ8LFNttYbo8nIydqi9iNCR8mvOkJ9\na0PH455OHkR69qO/ZwSRunD8XHwUCUKX2pe2Ya3v+Lzwa1qMrUR49GPGgFst+mKf3eFU7WmW73+b\n6uYarg+5htsiGj6QcQAAIABJREFUbzb7Wj+WtM98vfckH3x1CAc7NfffEseQAb5Kl6QoS+qNJZPg\n006Cj22y9d4YTUZO15VwuH1oLL/qCLUtdR2PuztqfzkipAsnQOPXI0HoUvqSXZ7HukOb0TeUo3V0\nY0rkRIb6D7aII1c9oaKxkuX736a4vpRk/0TmxEzHwYyn5lvaPpORr2fl5myaWwxMvz6SG4Zaz6T1\n7mZpvbFUEnzaSfCxTdKbzkwmE8X1pRyubAtBh6uOUNP8y/8fNwfXTpOl+7j6m+UIw8X0pbyhgo2H\nPyajfVjruuCR3NxvPC72tjffo66lnpUHVnGkupABukjuT7jTbMN7lrjPFBaf4dUNGVTXNjMmKYhZ\n46KwUyu/ynVPs8TeWCIJPu0k+Ngm6U3XTCYTpQ36jsnSh6uOUNVU3fG4q72GCM9+RHn2I1IXTrBb\nYLcEoa760mJs5evj3/LZsW20GFuI8OjLjAG39fphrQtpNrSwKvsDDuizCXYLZN6g+/AwwxpFlrrP\nVNQ08ur6DE6W1TEwwpsHJ8fh7Gh5i1Kak6X2xtJI8Gknwcc2SW8ujclkoryxotPQWHljZcfjLvbO\nRHj07RgaC3ELuqwVpX+rL9nlB1l/aBNl7cNat0XczLCAJJsd2vg1g9FAyqFN/Fi0C29nHQ8l/h5/\nTffOe7HkfaahqZUVm7LIOlpBqJ8bj00bhE5rnWtZXQyTyURVbTMlFfWUVTeQ0N8PDyc72R8uQIJP\nOwk+tkl6c+UqGis7DY2VNZR3POZk50i4R9+OobFQbfBFXRri130pb6hkY/7HZJRloULF6OBR3Bxu\nm8NaF2Iymfj02Fd8cvRL3BxceXDgPfTzCO227Vv6PtNqMPL+l4f4dn8ROq0Tj90+kFB/y16d+0Jq\nG1ooqainuKKeksp6SioaKKmop6SygaYWQ6fn9vHWMDwugBGx/nKa/2+Q4NNOgo9tkt50v6qm6k5H\nhErqyzoec1Q7EH7WEaEw95DzTsT9uS9tw1rf8dmxr2kxthDu0ZcZ/W8lWBvYk2/JKv14ahcfHkzF\nQW3PffFziPeJ6ZbtWsM+YzKZ+Gz3cdZ/U4CTox3zbo0nIdxb6bK61Njc2hZoKuvbQ04DpZVtYaeu\nsfWc5zvaq/HTafD3ciHAS4OX1okjJbXsyiqm1WAEoH+wByPiAxga7ScLPZ5Fgk87CT62SXpjftVN\nZ9rXEGoLQ6frSjoec1Db09c9tOOIUF/3MBztHPD11fJt3h7WH9pMaYMerYMbt0XKsNalOlCWzTvZ\n72MwGZkVfTsj+iRf8TataZ9JyyvlzY9zMBpNzLmhP6MHBylaT0urkbKqtqM1xWcduSmurKe69txL\nTdipVfh6uuCvc8HfS4O/l4aA9q89tU6of7Uv+PpqKTxRyZ6DpezMLibveBUA9nYqBkX6MDIugIQI\nb+ztbG/i99kk+LST4GObpDc9r7a5riMEHa46QlFtMSbaPmrsVXaEuYegddGwvzgHFar2s7VuQOMg\nh+0vx5HqY6zMWE1daz23hE/gxrAxVxQerW2fyT9VzWsbDlDb0MKEq0K5fXTEOYGhOxmNJvQ1jW1D\nURX1HUdxiivqKa9p5Ne/VVWAl7szAV4u+HlpCNBp2kOOCz4ezpd0dto5Q8TVjezMKWZHdglF+rZl\nKlyd7RkW48+IuAAigtxt8g8JCT7tJPjYJumN8upb6smvOtoxNHbiTBEmTDKs1Y2K60pYtv9tKpuq\nuDZoJNP6T7rss++scZ8prazn1fUHKK6oJ3mAL7+fGIujw6VPuv/Z2ZOKiyvrKa1o6Jh/U1bVQKvh\n3F+dHq6OHUduArw0+Ok0bWFH54KD/eXXcrbf6o3JZOJ4SS07sovZmVNCTV3b0SVfT2dGxAUwIi4A\nfy9Nt9RgDST4tJPgY5ukN5anobUBXFpwanYz+yrEtqSqqZrl+9+mqK6YRN8E7o6diYPdpc/7sNZ9\nprahheWpmRw8UUVEoDuPTB2Iu6vjBV9zsZOKATRO9u3BxgX/s47c+Os0uDiZ/9T6i+mNwWgk91gl\nO7KL2XuojOaWtvlA4YHujIgLYFiMH1pN1/9frJ0En3YSfGyT9MYySV/Mo76lgTcy13C46giRnv14\nIOHuSx5CtObetLQaWf1pLjuyS/DxcGb+9LbT3S9nUnGAV/u8G13bERx/LxfcXBwUHTq61N40Nrey\n75Cen7KLyTlWgcnUNq8oIdyb4XH+JEb6XNGRMUslwaedBB/bJL2xTNIX82kxtLAm5yP2lWUS6BrA\nvEH3onP2vOjXW3tvTCYTm384ypYfj2GnVmEwnvtr7udJxW1DUi7twUaDv87lvJOKLcWV9Kaqtold\nOSXsyC7meEktAC5OdgwZ4MfIuAD6h3pa7Pu+VBJ82knwsU3SG8skfTEvo8nIhsMf8+3JH9E5efJQ\n4n30cfW/qNf2lt78lHWaL3afQKtx6Dhy8/MwlfclTiq2FN3Vm1NltezILmFnTjEVNU0AeLk7MTw2\ngBFx/gT5ul3x91CSBJ92Enxsk/TGMklfzM9kMvHl8e1sLvgUjb0LDw68hwjPvhd8nfTGcnV3b4wm\nE4eOV/FTdjF7D5bS0NQ2rynU340RcQFcFeuPp5v1rYwtwaedBB/bJL2xTNKXnrPz9B7ez9uAnUrN\nPXGzGOQb3+XzpTeWy5y9aW4xsD9fz87sEjKPlGMwmlCpILavFyPjAhjc38dqro3WVfCxjncghBDi\nsg3vk4zWUctbWWt5M3MtMwbcxjVBw5UuS1gYRwc7hsX4MyzGn5r6ZtJyS9mRXUz20Qqyj1bg5GBH\nUn8fRsQFENNXZ5VDhSBHfLqN/IVkuaQ3lkn60vMKa07w74x3qG2p43d9x3Fzv/HnPUNJemO5lOhN\nSUU9O7KL2ZFdTFlVI9C2ZtFVsW2LJIb6u1ncIoky1NVOgo9tkt5YJumLMkrry1i2/23KGysY2WcY\nMwfchp268+nM0hvLpWRvTCYTBadq2JFdzO7cko6lAAJ9XBkR58/w2AC8PZwVqe3XJPi0k+Bjm6Q3\nlkn6opzqpjOsyHibE7VFJPjEcG/cbBztflnQTnpjuSylN60GI5kF5fyUXUxGvr5jJevoUE+GxwWQ\nPMAPjbNys2kUCz6LFy8mIyMDlUrFwoULGThw4DnPeeWVV9i/fz9r167tuK+xsZGJEycyb948pkyZ\nwunTp3nqqacwGAz4+vry8ssv4+joyJYtW1izZg1qtZrp06czbdq0LuuR4GObpDeWSfqirMbWRt7M\nXEte5WH6uYfx4KC7cXNwBaQ3lswSe1Pf2EJaXik7sks4dOLni6aqSYxqu2hqfLhXj180tavgY7ZK\ndu/eTWFhISkpKbzwwgu88MIL5zwnPz+ftLS0c+5fsWIFHh4eHbdfe+01Zs2axQcffEBYWBgbNmyg\nvr6e5cuXs3r1atauXcuaNWuoqqoy19sRQohexdnemT8Ouoeh/oM5WlPI/+39N+UNlUqXJayQxtmB\n6xKDWDA7iZceHMGUa8Px9XRmT14pr208wJ+W/ch7Xxyk4FQ1ljDIZLbgs2PHDsaNGwdAREQE1dXV\n1NbWdnrOiy++yPz58zvdV1BQQH5+PqNHj+64b9euXYwdOxaAMWPGsGPHDjIyMkhISECr1eLs7ExS\nUhLp6enmejtCCNHr2KvtuTN2BmNDr6WkvoxX9i7jVO1ppcsSVszH04WJI/vyt99fxXN3JzMuORi1\nCraln+KFtXt55o2dbP7hKKWV9YrVaLbgo9fr0el0Hbe9vLwoKyvruJ2amsqwYcMICgrq9LolS5aw\nYMGCTvc1NDTg6Ng2/uzt7U1ZWRl6vR4vL6/f3L4QQogLU6vUTImcyNTIiVQ3n+H/9q4gu/SQ0mUJ\nK6dSqegb4M6scf155eFRPD5tEMNj/ak608TmH46y4PWdvPNJriK19djMo7MPb1VVVZGamsqqVaso\nKSnpuH/Tpk0kJiYSEhJyUdu5mPvPptNpsLc338XYuhpTFMqS3lgm6YvlmOF7M8G+fizbtYa/ffsa\n9w+5g+vDRyldljgPa9xvAvw9GDu8L/WNLezMOs236adwc3NS5L2YLfj4+fmh1+s7bpeWluLr6wvA\nzp07qaioYPbs2TQ3N3P8+HEWL15MaWkpJ06cYPv27RQXF+Po6EhAQAAajYbGxkacnZ0pKSnBz8/v\nvNtPTEzssqZKMx5as8QJZ6KN9MYySV8sT3+XaB4edB9vZ7/PyrT3yDt9lCmRE8853V0opzfsNwlh\nOhLC2kaEzPVeFJncPGrUKD7//HMAsrOz8fPzw82t7aJnEyZMYOvWraxbt45ly5YRFxfHwoULefXV\nV9m4cSPr1q1j2rRpzJs3j5EjRzJy5MiObX3xxRdcc801DBo0iMzMTGpqaqirqyM9PZ3k5GRzvR0h\nhLAJ/XWRLB7/NAGu/mw/+SP/zniHuhbl5mMI0d3MdsQnKSmJuLg4Zs6ciUqlYtGiRaSmpqLVahk/\nfvwlbeuRRx7h6aefJiUlhcDAQG699VYcHBx44oknuO+++1CpVDz00ENotdZ3+E8IISxNgJsvfx7y\nEGtyPiRTn8vLe5by4MC7CbjIq7sLYclkAcNu0hsOP/ZW0hvLJH2xXD/3xmgy8t8jX/B54Tac7Zy4\nJ24W8T4xSpdn02S/uTiKDHUJIYSwbmqVmkkRE7gn9g4MJgMrD6zmy8LtFrEWixCXS4KPEEKILiUH\nDGZ+0h/xcHJnU8FW1uSk0GJoUbosIS6LBB8hhBAXFOYewlPJj9DXPZS0knT+uW8lVU3VSpclxCWT\n4COEEOKieDi58/jgBxgWkERhzQleSltKYc0JpcsS4pJI8BFCCHHRHOwcuDNmBrdF3kxN8xn+L30F\nacX7lC5LiIsmwUcIIcQlUalUjAu9jj8Ougd7lT2rcz5kc8GnGE1GpUsT4oIk+AghhLgscd7RPJn8\nMH4uPnxR+A2vH1hDQ2uj0mUJ0SUJPkIIIS5bgKsfTyY/TLQuiqzyXP6xZxml9foLv1AIhUjwEUII\ncUU0DhrmDbqXMSFXU1xfyst7lpJXcVjpsoQ4Lwk+Qgghrpid2o7boyYxO3oaTYZmlme8zfaTP8pi\nh8LiSPARQgjRbUYGDuWxwQ/gaq9h/aHNfHgwlVZjq9JlCdFBgo8QQohuFeHZl6eGPkKwWyA/Fu3i\ntX1vcqa5VumyhAAk+AghhDADL2cdfxoyj8F+AymoPspLe5Zy8kyR0mUJIcFHCCGEeTjZOXJf3Gwm\n9ruBisZKXkn/N/tLM5UuS9g4CT5CCCHMRqVS8bt+47g/4U4A3sxay9ajX8qkZ6EYCT5CCCHMLtE3\nnj8PeQgvZx2fHP2St7Pfp8nQrHRZwgZJ8BFCCNEjgtz68FTyI0R69mNf6QH+b++/qWisVLosYWMk\n+AghhOgxWkc3Hkm8n1GBV3Gytoglaa9RUHVM6bKEDZHgI4QQokfZq+25Y8AUpve/lfrWBv6173V+\nKkpTuixhIyT4CCGE6HEqlYrrgkfy8KDf42znxPt569lwaAsGo0Hp0kQvJ8FHCCGEYgZ4RfJk8iME\nuPrzzckf+HfGO9S31CtdlujFJPgIIYRQlK/Gmz8PeYh47xjyKg/z0p6lFNeVKF2W6KUk+AghhFCc\ni70zDwy8ixvCxlDWUM7Le5aTpc9VuizRC0nwEUIIYRHUKjWTI37H3bF3YDC1svLAar46/q0sdii6\nlQQfIYQQFmVowGDmJ/0Rd0ct/8n/hHdzU2gxtChdluglJPgIIYSwOGHuITw99FHC3EPYXZzOP/et\npLqpRumyRC8gwUcIIYRF8nByZ/7gBxkWkERhzQmWpL1GYc0JpcsSVk6CjxBCCIvlYOfAnTEzuDXi\nJmqaz/DP9BWkFe9TuixhxST4CCGEsGgqlYrxYaN5cODd2KnsWZ3zIZsLPsVoMipdmrBC9ubc+OLF\ni8nIyEClUrFw4UIGDhx4znNeeeUV9u/fz9q1a2loaGDBggWUl5fT1NTEvHnzGDNmDI8++iiVlW0X\nsquqqiIxMZEHHniAW265hfj4eAB0Oh2vvfaaOd+OEEIIBcX7xPBk8kOsPLCaLwq/4XRdMXfF3oGL\nvbPSpQkrYrbgs3v3bgoLC0lJSaGgoICFCxeSkpLS6Tn5+fmkpaXh4OAAwDfffEN8fDz3338/p06d\n4t5772XMmDGdAs0zzzzDtGnTAOjXrx9r164111sQQghhYQJc/Xkq+RHeznqfTH0u/9i7nAcT7sZX\n4610acJKmG2oa8eOHYwbNw6AiIgIqqurqa2t7fScF198kfnz53fcvummm7j//vsBOH36NP7+/p2e\nf+TIEc6cOXPeI0dCCCFsg8ZBw7xB9zIm+GqK60p4ec9SDlbkK12WsBJmO+Kj1+uJi4vruO3l5UVZ\nWRlubm4ApKamMmzYMIKCgs557cyZMykuLmblypWd7n/33XeZM2dOp+/x6KOPUlpayqxZs5g0aVKX\nNel0Guzt7a7kbXXJ11drtm2LKyO9sUzSF8tlDb35o/9sBhzpy5t7P2RZxlvcPXgaN0Zeh0qlUro0\ns7KG3lgys87xOdvZK29WVVWRmprKqlWrKCk593osH330Ebm5uTz55JNs2bIFlUpFc3Mze/fu5fnn\nnwfA09OTxx57jEmTJnHmzBmmTZvG8OHD8fPz+80aKivNd+E7X18tZWVnzLZ9cfmkN5ZJ+mK5rKk3\nCdqBPJqo5c3Md3knPYVDxceY1n8y9uoe+/XWo6ypN0rqKhyabajLz88PvV7fcbu0tBRfX18Adu7c\nSUVFBbNnz+bhhx8mOzubxYsXk5WVxenTpwGIiYnBYDBQUVEBQFpaWqchLjc3N6ZOnYqDgwNeXl7E\nx8dz5MgRc70dIYQQFirSsx9PD32UYLdAfijaxdL9b3KmufbCLxQ2yWzBZ9SoUXz++ecAZGdn4+fn\n1zHMNWHCBLZu3cq6detYtmwZcXFxLFy4kD179vDOO+8AbcNY9fX16HQ6ADIzM4mOju7Y/s6dO/n7\n3/8OQH19PXl5efTr189cb0cIIYQF83LW8ach8xjsm0B+1VFe2rOUU7WnlS5LWCCzHQtMSkoiLi6O\nmTNnolKpWLRoEampqWi1WsaPH3/e18ycOZNnn32WWbNm0djYyHPPPYda3ZbNysrKCA0N7XhucnIy\nmzZtYsaMGRgMBv7whz+cMxlaCCGE7XCyc+Te+Nl8duxrPjn6Jf/Yu5w50bcT4dkPO5Vd239qO+xU\nauxUdr1+LpA4P5XJhi57a85xURl3tVzSG8skfbFcvaE3+0ozeTfnI5qNv31xU7VKjZ1K3f7vucFI\nfdbXbY+d/2t1F4+1bf8Cj6t/+dqu4+tzn2entqd/cDAV5eabr9pbdDXHp3fO/hJCCGHTBvsl4Kfx\nYfuJH2kyNGEwGTGYDBhMBozGX742nP31WbebjS0YW35+rO1fS1gp2s3RlRhdf+J9Yoj1GoDGwUXp\nkqyOBB8hhBC9UpBbH2bH3N5t2zOZTBhNvxGafiNAGTs91n6f0dApiHW1nbNDWpOhmaM1haSV7COt\nZB9qlZoIj74k+MQS7xODv8a3295rbybBRwghhLgIKpWqbdgJ860HdyE+Pm7sO3qILH0umeU5HK46\nwuGqI6Tm/xc/Fx/ifWJI8IkhwqMfdmrl6rRkEnyEEEIIK6FSqQjRBhKiDeR3/cZS3XSG7PI8sspz\nya04xLYT37PtxPe42DsT6zWgbUjMewBuDq5Kl24xJPgIIYQQVsrDScvIwKGMDBxKi7GVw5UFZJXn\nkqnPZW9pBntLM1ChItwjrGNILEDjZ9NntMlZXd2kN5wF0VtJbyyT9MVySW8s18X2xmQycbquhEx9\nDlnluRytPo6Jtl/3Ps5exPvEEO8TQ5RneK9c5VrO6hJCCCFsiEqlItAtgEC3AG7sez1nmmvJKT9I\nZnkuueUH2X7yR7af/BFnOyeivfqT4BNDnHc0Wkc3pUs3Owk+QgghRC+ndXTjqj5DuKrPEFqNreRX\nHe0YEttflsn+skxUqOjrHkK8TywJPjEEugb0yiExCT5CCCGEDbFX2xPtFUW0VxRTI2+hpL6sY0js\nSHUhR2uO8/GRz9A5eZLgE0O8Tyz9PcNxsHNQuvRuIcFHCCGEsFEqlYoAVz8CXP0YHzaaupZ6csoP\nklWeS3b5Qb47tYPvTu3AUe3QaUjMw8ld6dIvmwQfIYQQQgDg6qBhaMBghgYMxmA0cKT6GJn6XLLK\nczmgz+aAPhuAMG0I8T7RJPjEEuwWaFVDYhJ8hBBCCHEOO7UdUboIonQRTImaSGl9WfvCiXnkVx2h\n8MwJPjn6JZ5OHsR5R5PgE8MAXSSOdo5Kl94lCT5CCCGEuCA/jS/Xh/pyfei11Lc0kFtxiEx9Ljnl\nefxYtIsfi3bhoLZngC6q7XR572h0zp5Kl30OCT5CCCGEuCQaBxeG+A9iiP8gjCYjR6uPd0yQ/vk/\ngBC3wPbLaMQSog1CrVIrXLkEHyGEEEJcAbVKTYRnXyI8+3Jr5E3oG8rJ0rddRuNQZQEnaov49NjX\nuDtqifeOJt4nhgG6KJztnRSpV4KPEEIIIbqNj4s3o0NGMTpkFI2tjeRVHO6YIP3T6TR+Op2Gvdqe\na4NGMDXqlh6vT4KPEEIIIczC2d6ZRL8EEv0SMJqMFNacJEufQ1Z5HtVNNYrUJMFHCCGEEGanVqnp\n5xFKP49QbomYoFwdin1nIYQQQogeJsFHCCGEEDZDgo8QQgghbIYEHyGEEELYDAk+QgghhLAZEnyE\nEEIIYTMk+AghhBDCZkjwEUIIIYTNkOAjhBBCCJshwUcIIYQQNkOCjxBCCCFshgQfIYQQQtgMCT5C\nCCGEsBkqk8lkUroIIYQQQoieIEd8hBBCCGEzJPgIIYQQwmZI8BFCCCGEzZDgI4QQQgibIcFHCCGE\nEDZDgo8QQgghbIYEn26wePFiZsyYwcyZMzlw4IDS5YizvPTSS8yYMYOpU6fyxRdfKF2OOEtjYyPj\nxo0jNTVV6VLEWbZs2cKkSZOYMmUK27dvV7ocAdTV1fHwww8zd+5cZs6cyffff690SVbNXukCrN3u\n3bspLCwkJSWFgoICFi5cSEpKitJlCWDnzp0cPnyYlJQUKisrue2227jhhhuULku0W7FiBR4eHkqX\nIc5SWVnJ8uXL2bhxI/X19SxdupTRo0crXZbN+89//kO/fv144oknKCkp4a677uKzzz5TuiyrJcHn\nCu3YsYNx48YBEBERQXV1NbW1tbi5uSlcmRg6dCgDBw4EwN3dnYaGBgwGA3Z2dgpXJgoKCsjPz5df\nqhZmx44djBgxAjc3N9zc3Pjf//1fpUsSgE6n4+DBgwDU1NSg0+kUrsi6yVDXFdLr9Z1+CL28vCgr\nK1OwIvEzOzs7NBoNABs2bODaa6+V0GMhlixZwoIFC5QuQ/zKyZMnaWxs5MEHH2TWrFns2LFD6ZIE\ncPPNN1NUVMT48eOZM2cOTz/9tNIlWTU54tPN5Aogluerr75iw4YNvPPOO0qXIoBNmzaRmJhISEiI\n0qWI86iqqmLZsmUUFRVx55138s0336BSqZQuy6Zt3ryZwMBA3n77bfLy8li4cKHMjbsCEnyukJ+f\nH3q9vuN2aWkpvr6+ClYkzvb999+zcuVK3nrrLbRardLlCGD79u2cOHGC7du3U1xcjKOjIwEBAYwc\nOVLp0myet7c3gwcPxt7entDQUFxdXamoqMDb21vp0mxaeno6V199NQDR0dGUlpbKsP0VkKGuKzRq\n1Cg+//xzALKzs/Hz85P5PRbizJkzvPTSS7z++ut4enoqXY5o9+qrr7Jx40bWrVvHtGnTmDdvnoQe\nC3H11Vezc+dOjEYjlZWV1NfXy3wSCxAWFkZGRgYAp06dwtXVVULPFZAjPlcoKSmJuLg4Zs6ciUql\nYtGiRUqXJNpt3bqVyspKHn/88Y77lixZQmBgoIJVCWG5/P39ufHGG5k+fToAf/nLX1Cr5e9jpc2Y\nMYOFCxcyZ84cWltbef7555UuyaqpTDIpRQghhBA2QqK8EEIIIWyGBB8hhBBC2AwJPkIIIYSwGRJ8\nhBBCCGEzJPgIIYQQwmZI8BFCWKSTJ08SHx/P3LlzO65K/cQTT1BTU3PR25g7dy4Gg+Gin3/HHXew\na9euyylXCGElJPgIISyWl5cXa9euZe3atXz00Uf4+fmxYsWKi3792rVrZaE3IUQnsoChEMJqDB06\nlJSUFPLy8liyZAmtra20tLTw3HPPERsby9y5c4mOjiY3N5c1a9YQGxtLdnY2zc3N/PWvf6W4uJjW\n1lYmT57MrFmzaGhoYP78+VRWVhIWFkZTUxMAJSUl/PnPfwagsbGRGTNmcPvttyv51oUQ3USCjxDC\nKhgMBr788kuGDBnCk08+yfLlywkNDT3noo0ajYb33nuv02vXrl2Lu7s7r7zyCo2Njdx0001cc801\n/PTTTzg7O5OSkkJpaSljx44F4NNPPyU8PJz/+Z//oampifXr1/f4+xVCmIcEHyGExaqoqGDu3LkA\nGI1GkpOTmTp1Kq+99hrPPvtsx/Nqa2sxGo1A22Vkfi0jI4MpU6YA4OzsTHx8PNnZ2Rw6dIghQ4YA\nbRccDg8PB+Caa67hgw8+YMGCBVx33XXMmDHDrO9TCNFzJPgIISzWz3N8znbmzBkcHBzOuf9nDg4O\n59ynUqk63TaZTKhUKkwmU6drUf0cniIiIvjkk09IS0vjs88+Y82aNXz00UdX+naEEBZAJjcLIayK\nVqslODiYb7/9FoCjR4+ybNmyLl8zaNAgvv/+ewDq6+vJzs4mLi6OiIgI9u3bB8Dp06c5evQoAB9/\n/DGZmZmMHDmSRYsWcfr0aVpbW834roQQPUWO+AghrM6SJUv429/+xhtvvEFraysLFizo8vlz587l\nr3/9K7Nnz6a5uZl58+YRHBzM5MmT2bZtG7NmzSI4OJiEhAQAIiMjWbRoEY6OjphMJu6//37s7eXj\nUojeQK6ijzuTAAAAT0lEQVTOLoQQQgibIUNdQgghhLAZEnyEEEIIYTMk+AghhBDCZkjwEUIIIYTN\nkOAjhBBCCJshwUcIIYQQNkOCjxBCCCFshgQfIYQQQtiM/w9PE+SwivLhqAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f48bcf4eed0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "JjBZ_q7aD9gh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Task 1: Can We Calculate LogLoss for These Predictions?\n",
        "\n",
        "**Examine the predictions and decide whether or not we can use them to calculate LogLoss.**\n",
        "\n",
        "`LinearRegressor` uses the L2 loss, which doesn't do a great job at penalizing misclassifications when the output is interpreted as a probability.  For example, there should be a huge difference whether a negative example is classified as positive with a probability of 0.9 vs 0.9999, but L2 loss doesn't strongly differentiate these cases.\n",
        "\n",
        "In contrast, `LogLoss` penalizes these \"confidence errors\" much more heavily.  Remember, `LogLoss` is defined as:\n",
        "\n",
        "$$Log Loss = \\sum_{(x,y)\\in D} -y \\cdot log(y_{pred}) - (1 - y) \\cdot log(1 - y_{pred})$$\n",
        "\n",
        "\n",
        "But first, we'll need to obtain the prediction values. We could use `LinearRegressor.predict` to obtain these.\n",
        "\n",
        "Given the predictions and that targets, can we calculate `LogLoss`?"
      ]
    },
    {
      "metadata": {
        "id": "dPpJUV862FYI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Solution\n",
        "\n",
        "Click below to display the solution."
      ]
    },
    {
      "metadata": {
        "id": "kXFQ5uig2RoP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "predict_validation_input_fn = lambda: my_input_fn(validation_examples, \n",
        "                                                  validation_targets[\"median_house_value_is_high\"], \n",
        "                                                  num_epochs=1, \n",
        "                                                  shuffle=False)\n",
        "\n",
        "validation_predictions = linear_regressor.predict(input_fn=predict_validation_input_fn)\n",
        "validation_predictions = np.array([item['predictions'][0] for item in validation_predictions])\n",
        "\n",
        "_ = plt.hist(validation_predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rYpy336F9wBg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Task 2: Train a Logistic Regression Model and Calculate LogLoss on the Validation Set\n",
        "\n",
        "To use logistic regression, simply use [LinearClassifier](https://www.tensorflow.org/api_docs/python/tf/estimator/LinearClassifier) instead of `LinearRegressor`. Complete the code below.\n",
        "\n",
        "**NOTE**: When running `train()` and `predict()` on a `LinearClassifier` model, you can access the real-valued predicted probabilities via the `\"probabilities\"` key in the returned dict—e.g., `predictions[\"probabilities\"]`. Sklearn's [log_loss](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html) function is handy for calculating LogLoss using these probabilities.\n"
      ]
    },
    {
      "metadata": {
        "id": "JElcb--E9wBm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_linear_classifier_model(\n",
        "    learning_rate,\n",
        "    steps,\n",
        "    batch_size,\n",
        "    training_examples,\n",
        "    training_targets,\n",
        "    validation_examples,\n",
        "    validation_targets):\n",
        "  \"\"\"Trains a linear classification model.\n",
        "  \n",
        "  In addition to training, this function also prints training progress information,\n",
        "  as well as a plot of the training and validation loss over time.\n",
        "  \n",
        "  Args:\n",
        "    learning_rate: A `float`, the learning rate.\n",
        "    steps: A non-zero `int`, the total number of training steps. A training step\n",
        "      consists of a forward and backward pass using a single batch.\n",
        "    batch_size: A non-zero `int`, the batch size.\n",
        "    training_examples: A `DataFrame` containing one or more columns from\n",
        "      `california_housing_dataframe` to use as input features for training.\n",
        "    training_targets: A `DataFrame` containing exactly one column from\n",
        "      `california_housing_dataframe` to use as target for training.\n",
        "    validation_examples: A `DataFrame` containing one or more columns from\n",
        "      `california_housing_dataframe` to use as input features for validation.\n",
        "    validation_targets: A `DataFrame` containing exactly one column from\n",
        "      `california_housing_dataframe` to use as target for validation.\n",
        "      \n",
        "  Returns:\n",
        "    A `LinearClassifier` object trained on the training data.\n",
        "  \"\"\"\n",
        "\n",
        "  periods = 10\n",
        "  steps_per_period = steps / periods\n",
        "  \n",
        "  # Create a linear classifier object.\n",
        "  my_optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
        "  my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)\n",
        "  linear_classifier = # YOUR CODE HERE: Construct the linear classifier.\n",
        "  \n",
        "  # Create input functions.\n",
        "  training_input_fn = lambda: my_input_fn(training_examples, \n",
        "                                          training_targets[\"median_house_value_is_high\"], \n",
        "                                          batch_size=batch_size)\n",
        "  predict_training_input_fn = lambda: my_input_fn(training_examples, \n",
        "                                                  training_targets[\"median_house_value_is_high\"], \n",
        "                                                  num_epochs=1, \n",
        "                                                  shuffle=False)\n",
        "  predict_validation_input_fn = lambda: my_input_fn(validation_examples, \n",
        "                                                    validation_targets[\"median_house_value_is_high\"], \n",
        "                                                    num_epochs=1, \n",
        "                                                    shuffle=False)\n",
        "  \n",
        "  # Train the model, but do so inside a loop so that we can periodically assess\n",
        "  # loss metrics.\n",
        "  print \"Training model...\"\n",
        "  print \"LogLoss (on training data):\"\n",
        "  training_log_losses = []\n",
        "  validation_log_losses = []\n",
        "  for period in range (0, periods):\n",
        "    # Train the model, starting from the prior state.\n",
        "    linear_classifier.train(\n",
        "        input_fn=training_input_fn,\n",
        "        steps=steps_per_period\n",
        "    )\n",
        "    # Take a break and compute predictions.    \n",
        "    training_probabilities = linear_classifier.predict(input_fn=predict_training_input_fn)\n",
        "    training_probabilities = np.array([item['probabilities'] for item in training_probabilities])\n",
        "    \n",
        "    validation_probabilities = linear_classifier.predict(input_fn=predict_validation_input_fn)\n",
        "    validation_probabilities = np.array([item['probabilities'] for item in validation_probabilities])\n",
        "    \n",
        "    training_log_loss = metrics.log_loss(training_targets, training_probabilities)\n",
        "    validation_log_loss = metrics.log_loss(validation_targets, validation_probabilities)\n",
        "    # Occasionally print the current loss.\n",
        "    print \"  period %02d : %0.2f\" % (period, training_log_loss)\n",
        "    # Add the loss metrics from this period to our list.\n",
        "    training_log_losses.append(training_log_loss)\n",
        "    validation_log_losses.append(validation_log_loss)\n",
        "  print \"Model training finished.\"\n",
        "  \n",
        "  # Output a graph of loss metrics over periods.\n",
        "  plt.ylabel(\"LogLoss\")\n",
        "  plt.xlabel(\"Periods\")\n",
        "  plt.title(\"LogLoss vs. Periods\")\n",
        "  plt.tight_layout()\n",
        "  plt.plot(training_log_losses, label=\"training\")\n",
        "  plt.plot(validation_log_losses, label=\"validation\")\n",
        "  plt.legend()\n",
        "\n",
        "  return linear_classifier\n",
        "\n",
        "linear_classifier = train_linear_classifier_model(\n",
        "    learning_rate=0.000005,\n",
        "    steps=500,\n",
        "    batch_size=20,\n",
        "    training_examples=training_examples,\n",
        "    training_targets=training_targets,\n",
        "    validation_examples=validation_examples,\n",
        "    validation_targets=validation_targets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VM0wmnFUIYH9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "linear_classifier = train_linear_classifier_model(\n",
        "    learning_rate=0.000005,\n",
        "    steps=500,\n",
        "    batch_size=20,\n",
        "    training_examples=training_examples,\n",
        "    training_targets=training_targets,\n",
        "    validation_examples=validation_examples,\n",
        "    validation_targets=validation_targets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "i2e3TlyL57Qs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Solution\n",
        "\n",
        "Click below to see the solution.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "5YxXd2hn6MuF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_linear_classifier_model(\n",
        "    learning_rate,\n",
        "    steps,\n",
        "    batch_size,\n",
        "    training_examples,\n",
        "    training_targets,\n",
        "    validation_examples,\n",
        "    validation_targets):\n",
        "  \"\"\"Trains a linear classification model.\n",
        "  \n",
        "  In addition to training, this function also prints training progress information,\n",
        "  as well as a plot of the training and validation loss over time.\n",
        "  \n",
        "  Args:\n",
        "    learning_rate: A `float`, the learning rate.\n",
        "    steps: A non-zero `int`, the total number of training steps. A training step\n",
        "      consists of a forward and backward pass using a single batch.\n",
        "    batch_size: A non-zero `int`, the batch size.\n",
        "    training_examples: A `DataFrame` containing one or more columns from\n",
        "      `california_housing_dataframe` to use as input features for training.\n",
        "    training_targets: A `DataFrame` containing exactly one column from\n",
        "      `california_housing_dataframe` to use as target for training.\n",
        "    validation_examples: A `DataFrame` containing one or more columns from\n",
        "      `california_housing_dataframe` to use as input features for validation.\n",
        "    validation_targets: A `DataFrame` containing exactly one column from\n",
        "      `california_housing_dataframe` to use as target for validation.\n",
        "      \n",
        "  Returns:\n",
        "    A `LinearClassifier` object trained on the training data.\n",
        "  \"\"\"\n",
        "\n",
        "  periods = 10\n",
        "  steps_per_period = steps / periods\n",
        "  \n",
        "  # Create a linear classifier object.\n",
        "  my_optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
        "  my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)  \n",
        "  linear_classifier = tf.estimator.LinearClassifier(\n",
        "      feature_columns=construct_feature_columns(training_examples),\n",
        "      optimizer=my_optimizer\n",
        "  )\n",
        "  \n",
        "  # Create input functions.\n",
        "  training_input_fn = lambda: my_input_fn(training_examples, \n",
        "                                          training_targets[\"median_house_value_is_high\"], \n",
        "                                          batch_size=batch_size)\n",
        "  predict_training_input_fn = lambda: my_input_fn(training_examples, \n",
        "                                                  training_targets[\"median_house_value_is_high\"], \n",
        "                                                  num_epochs=1, \n",
        "                                                  shuffle=False)\n",
        "  predict_validation_input_fn = lambda: my_input_fn(validation_examples, \n",
        "                                                    validation_targets[\"median_house_value_is_high\"], \n",
        "                                                    num_epochs=1, \n",
        "                                                    shuffle=False)\n",
        "  \n",
        "  # Train the model, but do so inside a loop so that we can periodically assess\n",
        "  # loss metrics.\n",
        "  print \"Training model...\"\n",
        "  print \"LogLoss (on training data):\"\n",
        "  training_log_losses = []\n",
        "  validation_log_losses = []\n",
        "  for period in range (0, periods):\n",
        "    # Train the model, starting from the prior state.\n",
        "    linear_classifier.train(\n",
        "        input_fn=training_input_fn,\n",
        "        steps=steps_per_period\n",
        "    )\n",
        "    # Take a break and compute predictions.    \n",
        "    training_probabilities = linear_classifier.predict(input_fn=predict_training_input_fn)\n",
        "    training_probabilities = np.array([item['probabilities'] for item in training_probabilities])\n",
        "    \n",
        "    validation_probabilities = linear_classifier.predict(input_fn=predict_validation_input_fn)\n",
        "    validation_probabilities = np.array([item['probabilities'] for item in validation_probabilities])\n",
        "    \n",
        "    training_log_loss = metrics.log_loss(training_targets, training_probabilities)\n",
        "    validation_log_loss = metrics.log_loss(validation_targets, validation_probabilities)\n",
        "    # Occasionally print the current loss.\n",
        "    print \"  period %02d : %0.2f\" % (period, training_log_loss)\n",
        "    # Add the loss metrics from this period to our list.\n",
        "    training_log_losses.append(training_log_loss)\n",
        "    validation_log_losses.append(validation_log_loss)\n",
        "  print \"Model training finished.\"\n",
        "  \n",
        "  # Output a graph of loss metrics over periods.\n",
        "  plt.ylabel(\"LogLoss\")\n",
        "  plt.xlabel(\"Periods\")\n",
        "  plt.title(\"LogLoss vs. Periods\")\n",
        "  plt.tight_layout()\n",
        "  plt.plot(training_log_losses, label=\"training\")\n",
        "  plt.plot(validation_log_losses, label=\"validation\")\n",
        "  plt.legend()\n",
        "\n",
        "  return linear_classifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UPM_T1FXsTaL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "linear_classifier = train_linear_classifier_model(\n",
        "    learning_rate=0.000005,\n",
        "    steps=500,\n",
        "    batch_size=20,\n",
        "    training_examples=training_examples,\n",
        "    training_targets=training_targets,\n",
        "    validation_examples=validation_examples,\n",
        "    validation_targets=validation_targets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "i-Xo83_aR6s_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Task 3: Calculate Accuracy and plot a ROC Curve for the Validation Set\n",
        "\n",
        "A few of the metrics useful for classification are the model [accuracy](https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification), the [ROC curve](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) and the area under the ROC curve (AUC). We'll examine these metrics.\n",
        "\n",
        "`LinearClassifier.evaluate` calculates useful metrics like accuracy and AUC."
      ]
    },
    {
      "metadata": {
        "id": "DKSQ87VVIYIA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "evaluation_metrics = linear_classifier.evaluate(input_fn=predict_validation_input_fn)\n",
        "\n",
        "print \"AUC on the validation set: %0.2f\" % evaluation_metrics['auc']\n",
        "print \"Accuracy on the validation set: %0.2f\" % evaluation_metrics['accuracy']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "47xGS2uNIYIE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You may use class probabilities, such as those calculated by `LinearClassifier.predict`,\n",
        "and Sklearn's [roc_curve](http://scikit-learn.org/stable/modules/model_evaluation.html#roc-metrics) to\n",
        "obtain the true positive and false positive rates needed to plot a ROC curve."
      ]
    },
    {
      "metadata": {
        "id": "xaU7ttj8IYIF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "validation_probabilities = linear_classifier.predict(input_fn=predict_validation_input_fn)\n",
        "# Get just the probabilities for the positive class.\n",
        "validation_probabilities = np.array([item['probabilities'][1] for item in validation_probabilities])\n",
        "\n",
        "false_positive_rate, true_positive_rate, thresholds = metrics.roc_curve(\n",
        "    validation_targets, validation_probabilities)\n",
        "plt.plot(false_positive_rate, true_positive_rate, label=\"our model\")\n",
        "plt.plot([0, 1], [0, 1], label=\"random classifier\")\n",
        "_ = plt.legend(loc=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PIdhwfgzIYII",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**See if you can tune the learning settings of the model trained at Task 2 to improve AUC.**\n",
        "\n",
        "Often times, certain metrics improve at the detriment of others, and you'll need to find the settings that achieve a good compromise.\n",
        "\n",
        "**Verify if all metrics improve at the same time.**"
      ]
    },
    {
      "metadata": {
        "id": "XKIqjsqcCaxO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TUNE THE SETTINGS BELOW TO IMPROVE AUC\n",
        "linear_classifier = train_linear_classifier_model(\n",
        "    learning_rate=0.000005,\n",
        "    steps=500,\n",
        "    batch_size=20,\n",
        "    training_examples=training_examples,\n",
        "    training_targets=training_targets,\n",
        "    validation_examples=validation_examples,\n",
        "    validation_targets=validation_targets)\n",
        "\n",
        "evaluation_metrics = linear_classifier.evaluate(input_fn=predict_validation_input_fn)\n",
        "\n",
        "print \"AUC on the validation set: %0.2f\" % evaluation_metrics['auc']\n",
        "print \"Accuracy on the validation set: %0.2f\" % evaluation_metrics['accuracy']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wCugvl0JdWYL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Solution\n",
        "\n",
        "Click below for a possible solution."
      ]
    },
    {
      "metadata": {
        "id": "VHosS1g2aetf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "One possible solution that works is to just train for longer, as long as we don't overfit. \n",
        "\n",
        "We can do this by increasing the number the steps, the batch size, or both.\n",
        "\n",
        "All metrics improve at the same time, so our loss metric is a good proxy\n",
        "for both AUC and accuracy.\n",
        "\n",
        "Notice how it takes many, many more iterations just to squeeze a few more \n",
        "units of AUC. This commonly happens. But often even this small gain is worth \n",
        "the costs."
      ]
    },
    {
      "metadata": {
        "id": "dWgTEYMddaA-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "linear_classifier = train_linear_classifier_model(\n",
        "    learning_rate=0.000003,\n",
        "    steps=20000,\n",
        "    batch_size=500,\n",
        "    training_examples=training_examples,\n",
        "    training_targets=training_targets,\n",
        "    validation_examples=validation_examples,\n",
        "    validation_targets=validation_targets)\n",
        "\n",
        "evaluation_metrics = linear_classifier.evaluate(input_fn=predict_validation_input_fn)\n",
        "\n",
        "print \"AUC on the validation set: %0.2f\" % evaluation_metrics['auc']\n",
        "print \"Accuracy on the validation set: %0.2f\" % evaluation_metrics['accuracy']"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}